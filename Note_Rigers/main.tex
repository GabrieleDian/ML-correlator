\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{booktabs}
\usepackage{url}
\usepackage[a4paper, top=1cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\title{GNN report}
\author{Rigers Aliaj}
\date{September 2025}

\begin{document}

\maketitle


\section{Data and features}
We use the denominator graph edges $7,8,9,10,11$ loops  as well as their corresponding coefficients to train a GNN on binary classification. The coefficient is either 1 or 0, depending on whether there is a numerator completion of the graph that contributes to the correlator. We input various features (as node features) into the model and see which lead to the best performance. The whole list of features considered follows
\begin{enumerate}
    \item Columns of the adjacency matrix
    \item Columns of identity matrix
    \item Laplacian eigenvectors with the 3 biggest eigenvalues
    \item  degree
   \item  betweenness
   \item clustering
   \item pagerank
   \item closeness
   \item face count
\end{enumerate}
Eventually features $3-9$ were the most expressive for the GNN. We ran the model with just the identity matrix columns as features to understand the importance of the other features. The experiments on 7,8 loops led to the model not even managing to learn the training set and no further experiments were made. Moreover, adding the identity columns to the set of features made the models perform worse.


\section{Runs and results}
\subsection{GIN}

\paragraph{Intro}In terms of the architecture, we almost exclusively work with a simple GINN (Graph Isomorphism Neural Network). This ensures that the model will treat equally graphs that are isomoprhic. However it might still fail to distinguish between non-isomorphic graphs that cannot be distinguished by the 1-WL graph isomorphism test.  This makes it as expressible as the  1-WL test. The architecture is expected to be ideal for graph level tasks with few nodes.

Based on features 3-9 we trained and tested the model for graphs of various loop orders. Since we deal with binary classification, the choice of a threshold affects heavily whether a graph will be identified with 0 or 1. Therefore, we present the ROC AUC and PR AUC metrics which are threshold independent. ROC AUC is the area under the curve of true positive rates (TPR) and false positive (FPR) rates, for different thresholds. These are defined as:
\begin{equation}
    \text{TPR}  =\frac{\text{TP}}{\text{TP}+\text{FN}}, \qquad
     \text{FPR}  =\frac{\text{FP}}{\text{FP}+\text{TN}} 
\end{equation}
The value ROC AUC
can be interpreted as follows: It gives the probability of a classifier to rank a randomly chosen denominator graph that contributes (coefficient=1) higher than one that does not contribute (coefficient=0). With the increase in the number of loops however, most of the graphs at hand do not contribute. For instance, at $11$-loop just 10 \% of the graphs have coefficient 1. In this case FPR becomes very small, since there are many TN resulting in an artificially large FPR. To overcome this problem we use PR AUC. This computes the area under the precision-recall curve for different thresholds. These are defined as\footnote{In our application we are interested in reducing the ansatz for the correlator, therefore including extra graphs in the ansatz that eventually do not contribute (recall on 0) is not as bad removing graphs that eventually contribute to the ansatz (recall on 1). Thus, instead of using ROC AUC and PR AUC we can optimize the threshold to maximize the recall on 1's.}
\begin{equation}
    \text{Precision}  =\frac{\text{TP}}{\text{TP}+\text{FP}}, \qquad
     \text{Recall}  =\frac{\text{TP}}{\text{TP}+\text{FN}} 
\end{equation}

Note that since we are mostly interested in minimizing FN while FP are not a problem, the performance of the model on an unbalanced set (negatives dominate) is better described by PR AUC. The reason is that these metrics focus on the minority class (TP,FN,FP) only.

\hspace{0.5pt}

\paragraph{Machines \& Architecture} The single loop runs are performed with 3 layers of 64 hidden channels. Then for testing on 10 loops we used 256 hidden channels and finally for testing on 11 loops we chose 512 hidden channels. Finally, the following choices have been made: learning rate: 0.0003, weight decay: 0.00003, dropout: 0.3, epochs=100.
The machine used for single-loop training has the following specs:
\begin{description}
    \item[CPU:]Intel(R) Core(TM) i5-8365U CPU @ 1.60GHz (8 cores)
    \item[GPU:]Intel Corporation WhiskeyLake-U GT2 [UHD Graphics 620] (rev 02)
    \item[RAM:] 15Gi.
\end{description}
The multi-loop ones were performed with the Maxwell Cluster with the following machines used:
\begin{description}
    \item[CPU:]96 cores, 754Gi RAM
    \item[GPU:] 4xNVIDIA L40S, 46068 MiB
    \end{description}



\paragraph{Results}Next follows a table with our findings. We use a 80-20 split for training and testing on single-loop experiments. For multi-loop experiments we take a random 80\% of the training set for training and we test on all test graphs.
The randomness of the chosen subset of graphs helps to avoid the shifting of the model towards special examples.


\begin{table}[htbp]
    \centering
    \caption{Experimental Results GIN}
    \label{tab:results}
    \begin{tabular}{ccccc}
        \toprule
 \textcolor{blue}{ROC}/\textcolor{OliveGreen}{PR} AUC (\%) & 8 $\rightarrow$ 8 & 9 $\rightarrow$ 9 & 7,8,9 $\rightarrow$ 10 & 7,8,9,10 $\rightarrow$ 11 \\
       \midrule
       \textbf{train} & \textcolor{blue}{96.0}/\textcolor{OliveGreen}{96.2} &  \textcolor{blue}{98.4}/\textcolor{OliveGreen}{97.7} &  \textcolor{blue}{99.0}/\textcolor{OliveGreen}{98.9} & \textcolor{blue}{99.9}/ \textcolor{OliveGreen}{99.7} \\
        \midrule
        \textbf{test} & \textcolor{blue}{85.8}/\textcolor{OliveGreen}{86.2} & \textcolor{blue}{95.8}/\textcolor{OliveGreen}{93.9} &  \textcolor{blue}{92.6}/\textcolor{OliveGreen}{86.4} & \textcolor{blue}{97.2}/ \textcolor{OliveGreen}{93.2} \\
        \midrule
        \textbf{time (s)} & {28} &{171} &  {720} & {5185} \\
        \bottomrule
    \end{tabular}
\end{table}
We observe that for the multiloop experiments (especially the one predicting 10-loop graph coefficients) the test metrics are not as good as the training ones. This is a typical sign of overfitting. In our binary classification we use \texttt{nn.BCEWithLogitsLoss} without any positive weight and with precision$=$0.5. The relative loss curves have the following forms:

\begin{figure}[htbp]
    \centering
    \begin{tabular}{cc}
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{Train_loss_7,8,9->10.png}
            \caption{Train $7,8,9\rightarrow 10$}
        \end{subfigure} &
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{Test_loss_7,8,9->10.png}
            \caption{Test $7,8,9\rightarrow 10$}
        \end{subfigure} \\
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{Train_loss_ 7,8,9,10->11.png}
            \caption{Train $7,8,9,10\rightarrow 11$}
        \end{subfigure} &
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{Test_loss_7,8,9,10->11.png}
            \caption{Test $7,8,9,10\rightarrow 11$}
        \end{subfigure}
    \end{tabular}
    \caption{Train and Test loss for each epoch.}
    \label{fig:box_edges}
\end{figure}
Indeed, the test loss is not reduced through training consistently, it rather fluctuates. However, all the relevant metrics (precision, accuracy, PR-AUC etc.) do increase on the test set through training. I am not sure what to make of this behaviour. One would imagine that an overfitted model will get increasingly better on the training set and increasingly worse on the test one, not just fluctuating.

Additionally we present the behaviour of accuracy, precision and recall for different thresholds and for different experiments below.
\begin{figure}[htbp]
    \centering
    \begin{tabular}{cc}
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{threshold_curves_8_to_8.png}
            \caption{$8\rightarrow 8$}
        \end{subfigure} &
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{threshold_curves_9_to_9.png}
            \caption{$9\rightarrow 9$}
        \end{subfigure} \\
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{threshold_curves_7_8_9_to_10.png}
            \caption{$7,8,9\rightarrow 10$}
        \end{subfigure} &
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{threshold_curves_7_8_9_10_to_11.png}
            \caption{$7,8,9,10\rightarrow 11$}
        \end{subfigure}
    \end{tabular}
    \caption{Accuracy, Precision and Recall as functions of the threshold for the GIN experiments.}
    \label{fig:box_edges}
\end{figure}

where, the accuracy is defined by
\begin{equation}
    \text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{P}+\text{N}}.
\end{equation}

As expected, Accuracy is not affected at all while Recall and Precision swap roles.
\subsection{PlanE}
Additionally we studied the performance of the model described in \url{https://arxiv.org/abs/2307.01180} on our dataset. This is representation learning model on planar graphs. It takes as input the graph structure and possible node or edge features. Based on these, it determines a representation through certain decompositions on the planar graph (BLOCKCUT, SPQR). The authros claim that any pair of graphs with $V_1,V_2$ number of vertices can be distinguished by a variant of their model with at most $\log_2(\max\{|V_1|,|V_2|\})+1$ layers.

Training for 50 epochs on a 80/10/10 split for training test and validation using 4 layers of 64 dimensions produced \footnote{These are lower loop cases where PR AUC and ROC AUC are not expected to differ much.}: 
\begin{table}[htbp]
    \centering
    \caption{Experimental Results PlanE}
    \label{tab:results}
    \begin{tabular}{cccc}
        \toprule
 ROC AUC (\%) & 7 $\rightarrow$ 7 & 8 $\rightarrow$ 8 & 9 $\rightarrow$ 9  \\
       \midrule
       \textbf{train} & 96.0 & 94.0 &  99.0  \\
        \midrule
        \textbf{test} & 87.3 & 78.8 &  85.5  \\
        \bottomrule
        \textbf{valid} & 87.3 & 79.4 &  85.3  \\

    \end{tabular}
\end{table}

It turns out that for our dataset PlanE does not perform significantly better at least for single loop cases. Given the much higher training time required for this model compared to our GIN we did not test on further cases.
\end{document}