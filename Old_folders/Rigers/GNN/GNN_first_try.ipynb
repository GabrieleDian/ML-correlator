{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn import Linear, ReLU, Sequential\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "#  Step 1: Load and parse CSVs\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edge_line(line):\n",
    "    # Split by comma only *outside* curly braces\n",
    "    edges_str = line.strip().split('\",\"')  # Handles entries like \"{1, 2}\",\"{1, 3}\",...\n",
    "    cleaned = [edge.replace('{', '(').replace('}', ')').replace('\"', '') for edge in edges_str]\n",
    "    return [tuple(ast.literal_eval(edge)) for edge in cleaned]\n",
    "\n",
    "# Load the file manually\n",
    "edges_per_graph = []\n",
    "with open(\"/home/rigers/Documents/GitHub/ML-correlator/Rigers/GNN/data_8_Loop/edges8Loop.csv\", 'r') as file:\n",
    "    for line in file:\n",
    "        edges = parse_edge_line(line)\n",
    "        edges_per_graph.append(edges)\n",
    "\n",
    "labels = []\n",
    "with open(\"/home/rigers/Documents/GitHub/ML-correlator/Rigers/GNN/data_8_Loop/coeffs8Loop.csv\", 'r') as f:\n",
    "    for line in f:\n",
    "        labels.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  Step 2: Convert to PyG Data objects\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "for edges, label in zip(edges_per_graph, labels):\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    num_nodes = edge_index.max().item() + 1\n",
    "    x = torch.eye(num_nodes)  # Identity features\n",
    "    y = torch.tensor([label], dtype=torch.long)\n",
    "    graph_list.append(Data(x=x, edge_index=edge_index, y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 3: Train/test split\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = train_test_split(graph_list, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 4: Define GIN model\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        nn1 = Sequential(Linear(num_node_features, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "\n",
    "        nn2 = Sequential(Linear(hidden_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "\n",
    "        self.linear = Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "#  Step 5: Train & Evaluate\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GIN(num_node_features=graph_list[0].x.size(1), hidden_dim=64, num_classes=len(set(labels))).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Loss: 34.6001\n",
      "[Epoch 002] Loss: 31.7933\n",
      "[Epoch 003] Loss: 31.9796\n",
      "[Epoch 004] Loss: 31.6843\n",
      "[Epoch 005] Loss: 31.8002\n",
      "[Epoch 006] Loss: 31.8384\n",
      "[Epoch 007] Loss: 32.0771\n",
      "[Epoch 008] Loss: 32.1821\n",
      "[Epoch 009] Loss: 32.0976\n",
      "[Epoch 010] Loss: 31.8674\n",
      "[Epoch 011] Loss: 31.4624\n",
      "[Epoch 012] Loss: 31.5245\n",
      "[Epoch 013] Loss: 31.6921\n",
      "[Epoch 014] Loss: 31.5753\n",
      "[Epoch 015] Loss: 31.2448\n",
      "[Epoch 016] Loss: 31.5451\n",
      "[Epoch 017] Loss: 31.8943\n",
      "[Epoch 018] Loss: 31.9082\n",
      "[Epoch 019] Loss: 31.7340\n",
      "[Epoch 020] Loss: 31.8192\n",
      "[Epoch 021] Loss: 31.3835\n",
      "[Epoch 022] Loss: 31.8150\n",
      "[Epoch 023] Loss: 31.5621\n",
      "[Epoch 024] Loss: 31.9390\n",
      "[Epoch 025] Loss: 31.5413\n",
      "[Epoch 026] Loss: 31.6681\n",
      "[Epoch 027] Loss: 31.4917\n",
      "[Epoch 028] Loss: 31.7837\n",
      "[Epoch 029] Loss: 31.8234\n",
      "[Epoch 030] Loss: 31.5585\n",
      "[Epoch 031] Loss: 31.7601\n",
      "[Epoch 032] Loss: 31.8431\n",
      "[Epoch 033] Loss: 31.6665\n",
      "[Epoch 034] Loss: 31.6741\n",
      "[Epoch 035] Loss: 31.6706\n",
      "[Epoch 036] Loss: 31.8154\n",
      "[Epoch 037] Loss: 31.6420\n",
      "[Epoch 038] Loss: 31.5322\n",
      "[Epoch 039] Loss: 31.3023\n",
      "[Epoch 040] Loss: 31.8172\n",
      "[Epoch 041] Loss: 31.4624\n",
      "[Epoch 042] Loss: 31.6810\n",
      "[Epoch 043] Loss: 31.5579\n",
      "[Epoch 044] Loss: 31.3938\n",
      "[Epoch 045] Loss: 31.6931\n",
      "[Epoch 046] Loss: 31.2983\n",
      "[Epoch 047] Loss: 31.4864\n",
      "[Epoch 048] Loss: 31.3878\n",
      "[Epoch 049] Loss: 31.5580\n",
      "[Epoch 050] Loss: 31.5224\n",
      "[Epoch 051] Loss: 31.7633\n",
      "[Epoch 052] Loss: 31.5840\n",
      "[Epoch 053] Loss: 31.9544\n",
      "[Epoch 054] Loss: 31.9686\n",
      "[Epoch 055] Loss: 31.8132\n",
      "[Epoch 056] Loss: 31.7221\n",
      "[Epoch 057] Loss: 31.6432\n",
      "[Epoch 058] Loss: 31.7306\n",
      "[Epoch 059] Loss: 31.6513\n",
      "[Epoch 060] Loss: 31.7587\n",
      "[Epoch 061] Loss: 31.6168\n",
      "[Epoch 062] Loss: 31.4349\n",
      "[Epoch 063] Loss: 31.5485\n",
      "[Epoch 064] Loss: 31.5459\n",
      "[Epoch 065] Loss: 31.7119\n",
      "[Epoch 066] Loss: 32.0271\n",
      "[Epoch 067] Loss: 31.8880\n",
      "[Epoch 068] Loss: 31.6225\n",
      "[Epoch 069] Loss: 31.4899\n",
      "[Epoch 070] Loss: 31.9810\n",
      "[Epoch 071] Loss: 31.5415\n",
      "[Epoch 072] Loss: 31.9814\n",
      "[Epoch 073] Loss: 32.0746\n",
      "[Epoch 074] Loss: 31.7071\n",
      "[Epoch 075] Loss: 31.4248\n",
      "[Epoch 076] Loss: 31.5180\n",
      "[Epoch 077] Loss: 31.8942\n",
      "[Epoch 078] Loss: 32.0599\n",
      "[Epoch 079] Loss: 31.8185\n",
      "[Epoch 080] Loss: 32.0413\n",
      "[Epoch 081] Loss: 31.5992\n",
      "[Epoch 082] Loss: 31.7102\n",
      "[Epoch 083] Loss: 31.9683\n",
      "[Epoch 084] Loss: 31.4076\n",
      "[Epoch 085] Loss: 31.6423\n",
      "[Epoch 086] Loss: 31.4941\n",
      "[Epoch 087] Loss: 31.9037\n",
      "[Epoch 088] Loss: 31.6531\n",
      "[Epoch 089] Loss: 32.2416\n",
      "[Epoch 090] Loss: 31.6765\n",
      "[Epoch 091] Loss: 31.4471\n",
      "[Epoch 092] Loss: 31.7272\n",
      "[Epoch 093] Loss: 31.6959\n",
      "[Epoch 094] Loss: 31.7187\n",
      "[Epoch 095] Loss: 31.6506\n",
      "[Epoch 096] Loss: 31.9459\n",
      "[Epoch 097] Loss: 31.7126\n",
      "[Epoch 098] Loss: 31.6866\n",
      "[Epoch 099] Loss: 31.8668\n",
      "[Epoch 100] Loss: 31.8630\n",
      "[Epoch 101] Loss: 31.8287\n",
      "[Epoch 102] Loss: 31.9478\n",
      "[Epoch 103] Loss: 31.6270\n",
      "[Epoch 104] Loss: 31.3267\n",
      "[Epoch 105] Loss: 31.3896\n",
      "[Epoch 106] Loss: 31.6387\n",
      "[Epoch 107] Loss: 31.8339\n",
      "[Epoch 108] Loss: 31.6545\n",
      "[Epoch 109] Loss: 31.4476\n",
      "[Epoch 110] Loss: 31.5815\n",
      "[Epoch 111] Loss: 31.6452\n",
      "[Epoch 112] Loss: 31.7056\n",
      "[Epoch 113] Loss: 31.4140\n",
      "[Epoch 114] Loss: 31.7233\n",
      "[Epoch 115] Loss: 31.3357\n",
      "[Epoch 116] Loss: 31.5687\n",
      "[Epoch 117] Loss: 31.7472\n",
      "[Epoch 118] Loss: 32.0084\n",
      "[Epoch 119] Loss: 31.5510\n",
      "[Epoch 120] Loss: 31.4841\n",
      "[Epoch 121] Loss: 31.7524\n",
      "[Epoch 122] Loss: 31.4326\n",
      "[Epoch 123] Loss: 31.5492\n",
      "[Epoch 124] Loss: 31.6242\n",
      "[Epoch 125] Loss: 31.5413\n",
      "[Epoch 126] Loss: 31.6761\n",
      "[Epoch 127] Loss: 31.7997\n",
      "[Epoch 128] Loss: 31.4957\n",
      "[Epoch 129] Loss: 32.4495\n",
      "[Epoch 130] Loss: 31.9282\n",
      "[Epoch 131] Loss: 31.5569\n",
      "[Epoch 132] Loss: 31.6330\n",
      "[Epoch 133] Loss: 31.4266\n",
      "[Epoch 134] Loss: 31.5916\n",
      "[Epoch 135] Loss: 31.6141\n",
      "[Epoch 136] Loss: 31.9925\n",
      "[Epoch 137] Loss: 31.7929\n",
      "[Epoch 138] Loss: 32.6763\n",
      "[Epoch 139] Loss: 31.7562\n",
      "[Epoch 140] Loss: 31.7070\n",
      "[Epoch 141] Loss: 31.6240\n",
      "[Epoch 142] Loss: 31.6206\n",
      "[Epoch 143] Loss: 31.7584\n",
      "[Epoch 144] Loss: 31.8344\n",
      "[Epoch 145] Loss: 32.2488\n",
      "[Epoch 146] Loss: 31.6379\n",
      "[Epoch 147] Loss: 31.8060\n",
      "[Epoch 148] Loss: 31.6118\n",
      "[Epoch 149] Loss: 31.6258\n",
      "[Epoch 150] Loss: 31.5634\n",
      "[Epoch 151] Loss: 31.4483\n",
      "[Epoch 152] Loss: 31.6013\n",
      "[Epoch 153] Loss: 31.5373\n",
      "[Epoch 154] Loss: 31.6453\n",
      "[Epoch 155] Loss: 31.3713\n",
      "[Epoch 156] Loss: 31.4320\n",
      "[Epoch 157] Loss: 31.7821\n",
      "[Epoch 158] Loss: 32.1829\n",
      "[Epoch 159] Loss: 31.4557\n",
      "[Epoch 160] Loss: 31.6895\n",
      "[Epoch 161] Loss: 31.4879\n",
      "[Epoch 162] Loss: 31.4330\n",
      "[Epoch 163] Loss: 31.6406\n",
      "[Epoch 164] Loss: 31.6576\n",
      "[Epoch 165] Loss: 31.4818\n",
      "[Epoch 166] Loss: 32.0161\n",
      "[Epoch 167] Loss: 31.7963\n",
      "[Epoch 168] Loss: 31.5415\n",
      "[Epoch 169] Loss: 31.8111\n",
      "[Epoch 170] Loss: 31.7667\n",
      "[Epoch 171] Loss: 31.6772\n",
      "[Epoch 172] Loss: 31.4563\n",
      "[Epoch 173] Loss: 31.5724\n",
      "[Epoch 174] Loss: 31.4936\n",
      "[Epoch 175] Loss: 31.7373\n",
      "[Epoch 176] Loss: 31.7131\n",
      "[Epoch 177] Loss: 31.5659\n",
      "[Epoch 178] Loss: 31.4718\n",
      "[Epoch 179] Loss: 31.6451\n",
      "[Epoch 180] Loss: 31.5526\n",
      "[Epoch 181] Loss: 31.7793\n",
      "[Epoch 182] Loss: 32.1084\n",
      "[Epoch 183] Loss: 31.6533\n",
      "[Epoch 184] Loss: 31.9343\n",
      "[Epoch 185] Loss: 31.9842\n",
      "[Epoch 186] Loss: 31.4434\n",
      "[Epoch 187] Loss: 31.6471\n",
      "[Epoch 188] Loss: 31.5063\n",
      "[Epoch 189] Loss: 31.4416\n",
      "[Epoch 190] Loss: 31.6372\n",
      "[Epoch 191] Loss: 31.5222\n",
      "[Epoch 192] Loss: 32.0070\n",
      "[Epoch 193] Loss: 31.5565\n",
      "[Epoch 194] Loss: 31.7004\n",
      "[Epoch 195] Loss: 31.7106\n",
      "[Epoch 196] Loss: 31.7332\n",
      "[Epoch 197] Loss: 31.7468\n",
      "[Epoch 198] Loss: 31.5076\n",
      "[Epoch 199] Loss: 31.6097\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop\n",
    "for epoch in range(1, 200):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch:03d}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8147\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += (pred == data.y).sum().item()\n",
    "    total += data.num_graphs\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
