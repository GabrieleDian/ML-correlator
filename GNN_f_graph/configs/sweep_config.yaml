# Simple WandB Sweep Configuration
# Good starting point for hyperparameter search
project: "f_graph_gnn_loop_7"
program: one_run_simple_f.py
method: random  # Random search is simple and effective
metric:
  name: train_recall
  goal: maximize

parameters:
  # Base config file
  config:
    value: configs/config.yaml
  # Learning rate
  learning_rate:
    distribution: log_uniform_values
    min: 0.0001
    max: 0.1
  # Weight decay (L2 regularization)
  weight_decay:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.01
  # Hidden layer size
  hidden_channels:
    values: [64, 128, 256]
  # Number of GNN layers
  num_layers:
    values: [2, 3]
  # Dropout rate
  dropout:
    values: [0.3, 0.4, 0.5]
  # Batch size
  batch_size:
    values: [64, 128, 256]