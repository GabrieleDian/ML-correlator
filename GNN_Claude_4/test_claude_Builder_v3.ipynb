{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb101a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example workflow demonstrating how to use the improved GraphBuilder\n",
    "for planar graph classification with 11-14 nodes\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool,global_add_pool, GraphNorm\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarGraphGNN(torch.nn.Module):\n",
    "    \"\"\"Simple GNN for planar graph classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, hidden_dim=32, num_classes=2, dropout=0.2):\n",
    "        super(PlanarGraphGNN, self).__init__()\n",
    "        \n",
    "        # Use shallow architecture for small graphs\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.classifier = torch.nn.Linear(hidden_dim//2, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        if batch is None:\n",
    "            x = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long))\n",
    "        else:\n",
    "            x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd26673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(graphs_data: List[Tuple[List, int]], \n",
    "                        feature_config: Dict) -> List[Data]:\n",
    "    \"\"\"\n",
    "    Create dataset from graph data using improved GraphBuilder\n",
    "    \n",
    "    Args:\n",
    "        graphs_data: List of (edges, label) tuples\n",
    "        feature_config: Configuration for GraphBuilder\n",
    "        \n",
    "    Returns:\n",
    "        List of PyTorch Geometric Data objects\n",
    "    \"\"\"\n",
    "    from GraphBuilder_with_features_Claude import GraphBuilder\n",
    "    \n",
    "    dataset = []\n",
    "    all_features = []\n",
    "    \n",
    "    # First pass: collect all features for normalization\n",
    "    print(\"Extracting features...\")\n",
    "    for edges, label in graphs_data:\n",
    "        builder = GraphBuilder(\n",
    "            solid_edges=edges,\n",
    "            coeff=label,\n",
    "            **feature_config\n",
    "        )\n",
    "        data = builder.build()\n",
    "        dataset.append(data)\n",
    "        all_features.append(data.x.numpy())\n",
    "    \n",
    "    # Compute normalization statistics\n",
    "    all_features = np.vstack(all_features)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_features)\n",
    "    \n",
    "    # Second pass: normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    for i, data in enumerate(dataset):\n",
    "        # Get the starting index for this graph's features\n",
    "        start_idx = sum(d.num_nodes for d in dataset[:i])\n",
    "        end_idx = start_idx + data.num_nodes\n",
    "        \n",
    "        # Normalize\n",
    "        normalized_features = scaler.transform(data.x.numpy())\n",
    "        data.x = torch.FloatTensor(normalized_features)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset)} graphs\")\n",
    "    print(f\"Feature dimensions: {dataset[0].x.shape[1]}\")\n",
    "    print(f\"Feature names: {dataset[0].feature_names}\")\n",
    "    \n",
    "    return dataset, scaler\n",
    "\n",
    "\n",
    "def evaluate_feature_importance(model, dataset, feature_names):\n",
    "    \"\"\"Evaluate which features are most important for the model\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    feature_gradients = []\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        for data in dataset:\n",
    "            data.x.requires_grad = True\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = F.nll_loss(out, data.y.unsqueeze(0))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Store gradients\n",
    "            grad = data.x.grad.abs().mean(dim=0)\n",
    "            feature_gradients.append(grad.numpy())\n",
    "    \n",
    "    # Average gradients across all graphs\n",
    "    avg_gradients = np.mean(feature_gradients, axis=0)\n",
    "    \n",
    "    # Sort features by importance\n",
    "    importance_scores = list(zip(feature_names, avg_gradients))\n",
    "    importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "def train_and_evaluate(dataset, feature_config, num_epochs=100, k_folds=5):\n",
    "    \"\"\"Train and evaluate model using k-fold cross validation with learning rate scheduling\"\"\"\n",
    "    \n",
    "    # Prepare for k-fold cross validation\n",
    "    labels = [data.y.item() for data in dataset]\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_feature_importance = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset)), labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset = [dataset[i] for i in train_idx]\n",
    "        test_dataset = [dataset[i] for i in test_idx]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        num_features = dataset[0].x.shape[1]\n",
    "        model = PlanarGraphGNN(num_features, hidden_dim=32, num_classes=2)\n",
    "        \n",
    "        # Initialize optimizer with a higher initial learning rate\n",
    "        # The scheduler will handle the reduction\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        # ReduceLROnPlateau: Reduces LR when validation loss plateaus\n",
    "        #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #    optimizer, \n",
    "        #    mode='min',  # Minimize loss\n",
    "        #    factor=0.5,  # Reduce LR by half\n",
    "        #    patience=10,  # Wait 10 epochs before reducing\n",
    "        #    min_lr=1e-5,  # Minimum LR\n",
    "        #)\n",
    "        \n",
    "        # Alternative schedulers you could use:\n",
    "        # 1. CosineAnnealingLR - smooth cosine decay\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        #     optimizer, T_max=num_epochs, eta_min=1e-5\n",
    "        # )\n",
    "        \n",
    "        # 2. StepLR - step decay at fixed intervals\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        #     optimizer, step_size=30, gamma=0.5\n",
    "        # )\n",
    "        \n",
    "        # 3. OneCycleLR - one cycle policy (very effective for small datasets)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.1, epochs=num_epochs, \n",
    "            steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        early_stop_patience = 20\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                loss = F.nll_loss(out, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Use train dataset as validation since we have limited data\n",
    "                for batch in train_loader:\n",
    "                    out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                    val_loss += F.nll_loss(out, batch.y).item()\n",
    "                    pred = out.argmax(dim=1)\n",
    "                    correct += pred.eq(batch.y).sum().item()\n",
    "                    total += batch.y.size(0)\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(train_loader)\n",
    "            val_acc = correct / total\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # For OneCycleLR, call scheduler.step() after each batch instead\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"  Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Final evaluation on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += pred.eq(batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        fold_accuracies.append(accuracy)\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Feature importance for this fold\n",
    "        if hasattr(dataset[0], 'feature_names'):\n",
    "            importance = evaluate_feature_importance(model, test_dataset, \n",
    "                                                   dataset[0].feature_names)\n",
    "            fold_feature_importance.append(importance)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cross-validation Results:\")\n",
    "    print(f\"  Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    print(f\"  All Folds: {fold_accuracies}\")\n",
    "    \n",
    "    # Average feature importance across folds\n",
    "    if fold_feature_importance:\n",
    "        avg_importance = {}\n",
    "        for feat_name in dataset[0].feature_names:\n",
    "            scores = [dict(fold)[feat_name] for fold in fold_feature_importance]\n",
    "            avg_importance[feat_name] = np.mean(scores)\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        sorted_importance = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (feat, score) in enumerate(sorted_importance[:10]):\n",
    "            print(f\"  {i+1}. {feat:25s}: {score:.4f}\")\n",
    "    \n",
    "    return fold_accuracies, avg_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ac6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_feature_sets(graphs_data):\n",
    "    \"\"\"Compare different feature configurations\"\"\"\n",
    "    \n",
    "    feature_configs = {\n",
    "        'Minimal': {\n",
    "            'selected_features': ['basic', 'face'],\n",
    "            'laplacian_pe_k': 0\n",
    "        },\n",
    "        'Planar-focused': {\n",
    "            'selected_features': ['basic', 'face', 'dual'],\n",
    "            'laplacian_pe_k': 2\n",
    "        },\n",
    "        'Balanced': {\n",
    "            'selected_features': ['basic', 'face', 'spectral_node', 'centrality'],\n",
    "            'laplacian_pe_k': 3\n",
    "        },\n",
    "        'Full': {\n",
    "            'selected_features': ['basic', 'face', 'spectral_node', 'dual', 'centrality'],\n",
    "            'laplacian_pe_k': 4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, config in feature_configs.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing configuration: {config_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create dataset with this configuration\n",
    "        dataset, scaler = create_graph_dataset(graphs_data, config)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        accuracies, importance = train_and_evaluate(dataset, config, num_epochs=50)\n",
    "        \n",
    "        results[config_name] = {\n",
    "            'accuracies': accuracies,\n",
    "            'mean_accuracy': np.mean(accuracies),\n",
    "            'std_accuracy': np.std(accuracies),\n",
    "            'num_features': dataset[0].x.shape[1],\n",
    "            'importance': importance\n",
    "        }\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    configs = list(results.keys())\n",
    "    means = [results[c]['mean_accuracy'] for c in configs]\n",
    "    stds = [results[c]['std_accuracy'] for c in configs]\n",
    "    num_features = [results[c]['num_features'] for c in configs]\n",
    "    \n",
    "    x = np.arange(len(configs))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(x, means, yerr=stds, capsize=10)\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance by Feature Configuration')\n",
    "    plt.xticks(x, configs, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(num_features, means, s=100)\n",
    "    plt.errorbar(num_features, means, yerr=stds, fmt='none', capsize=5)\n",
    "    for i, config in enumerate(configs):\n",
    "        plt.annotate(config, (num_features[i], means[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Feature Dimensionality')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94a19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating example planar graphs...\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "#print(\"Generating example planar graphs...\")\n",
    "#graphs_data = generate_example_graphs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5caecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop=8\n",
    "# Create the edge and y lists from the csv files\\\n",
    "edges=[]\n",
    "y=[]\n",
    "for i in range(loop,loop+1):\n",
    "    filename = f'../Graph_Edge_Data/den_graph_data_{loop}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    edges += df['EDGES'].tolist()\n",
    "    y += df['COEFFICIENTS'].tolist()\n",
    "edges = [ast.literal_eval(e) for e in edges]    \n",
    "graphs_data = list(zip(edges, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502fe948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing configuration: Minimal\n",
      "============================================================\n",
      "Extracting features...\n",
      "Normalizing features...\n",
      "Created dataset with 1432 graphs\n",
      "Feature dimensions: 5\n",
      "Feature names: ['degree', 'num_faces', 'avg_face_size', 'max_face_size', 'face_size_variance']\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/50, Train Loss: 0.6968, Val Loss: 0.6946, Val Acc: 0.5092, LR: 0.100000\n",
      "  Epoch 40/50, Train Loss: 0.6956, Val Loss: 0.6938, Val Acc: 0.5092, LR: 0.025000\n",
      "  Test Accuracy: 0.5087\n",
      "\n",
      "Fold 2/5\n",
      "  Epoch 20/50, Train Loss: 0.6940, Val Loss: 0.6941, Val Acc: 0.5092, LR: 0.050000\n",
      "  Early stopping at epoch 21\n",
      "  Test Accuracy: 0.5087\n",
      "\n",
      "Fold 3/5\n",
      "  Epoch 20/50, Train Loss: 0.6946, Val Loss: 0.6936, Val Acc: 0.4913, LR: 0.050000\n",
      "  Early stopping at epoch 24\n",
      "  Test Accuracy: 0.5105\n",
      "\n",
      "Fold 4/5\n",
      "  Epoch 20/50, Train Loss: 0.6940, Val Loss: 0.6930, Val Acc: 0.5087, LR: 0.050000\n",
      "  Early stopping at epoch 39\n",
      "  Test Accuracy: 0.5105\n",
      "\n",
      "Fold 5/5\n",
      "  Epoch 20/50, Train Loss: 0.6472, Val Loss: 0.6190, Val Acc: 0.6632, LR: 0.100000\n",
      "  Epoch 40/50, Train Loss: 0.6223, Val Loss: 0.6009, Val Acc: 0.6632, LR: 0.050000\n",
      "  Test Accuracy: 0.6049\n",
      "\n",
      "==================================================\n",
      "Cross-validation Results:\n",
      "  Mean Accuracy: 0.5287 ± 0.0381\n",
      "  All Folds: [0.5087108013937283, 0.5087108013937283, 0.5104895104895105, 0.5104895104895105, 0.6048951048951049]\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "  1. degree                   : 0.0307\n",
      "  2. max_face_size            : 0.0162\n",
      "  3. face_size_variance       : 0.0121\n",
      "  4. avg_face_size            : 0.0017\n",
      "  5. num_faces                : 0.0013\n",
      "\n",
      "============================================================\n",
      "Testing configuration: Planar-focused\n",
      "============================================================\n",
      "Extracting features...\n",
      "Normalizing features...\n",
      "Created dataset with 1432 graphs\n",
      "Feature dimensions: 10\n",
      "Feature names: ['degree', 'num_faces', 'avg_face_size', 'max_face_size', 'face_size_variance', 'dual_degree', 'dual_clustering', 'dual_degree_ratio', 'dual_betweenness', 'face_edge_ratio']\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/50, Train Loss: 0.6855, Val Loss: 0.6937, Val Acc: 0.5100, LR: 0.100000\n",
      "  Early stopping at epoch 30\n",
      "  Test Accuracy: 0.5122\n",
      "\n",
      "Fold 2/5\n",
      "  Epoch 20/50, Train Loss: 0.6612, Val Loss: 0.6555, Val Acc: 0.6341, LR: 0.050000\n",
      "  Early stopping at epoch 25\n",
      "  Test Accuracy: 0.5854\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compare different feature configurations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mcompare_feature_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 274\u001b[39m, in \u001b[36mcompare_feature_sets\u001b[39m\u001b[34m(graphs_data)\u001b[39m\n\u001b[32m    271\u001b[39m     dataset, scaler = create_graph_dataset(graphs_data, config)\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     accuracies, importance = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     results[config_name] = {\n\u001b[32m    277\u001b[39m         \u001b[33m'\u001b[39m\u001b[33maccuracies\u001b[39m\u001b[33m'\u001b[39m: accuracies,\n\u001b[32m    278\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmean_accuracy\u001b[39m\u001b[33m'\u001b[39m: np.mean(accuracies),\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: importance\n\u001b[32m    282\u001b[39m     }\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Plot comparison\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(dataset, feature_config, num_epochs, k_folds)\u001b[39m\n\u001b[32m    160\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# Use train dataset as validation since we have limited data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/loader/dataloader.py:27\u001b[39m, in \u001b[36mCollater.__call__\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     25\u001b[39m elem = batch[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch.Tensor):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/data/batch.py:97\u001b[39m, in \u001b[36mBatch.from_data_list\u001b[39m\u001b[34m(cls, data_list, follow_batch, exclude_keys)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_data_list\u001b[39m(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     88\u001b[39m ) -> Self:\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     batch, slice_dict, inc_dict = \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     batch._num_graphs = \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    107\u001b[39m     batch._slice_dict = slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/data/collate.py:109\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m value, slices, incs = \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value.is_cuda:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/data/collate.py:166\u001b[39m, in \u001b[36m_collate\u001b[39m\u001b[34m(key, values, data_list, stores, increment)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m elem.dim() == \u001b[32m0\u001b[39m:\n\u001b[32m    165\u001b[39m     values = [value.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m sizes = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m slices = cumsum(sizes)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m increment:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Compare different feature configurations\n",
    "results = compare_feature_sets(graphs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8cca9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6627 ± 0.0105\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7081 ± 0.0319\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7388 ± 0.0286\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7681 ± 0.0143\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ba914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6836 ± 0.0247\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7479 ± 0.0072\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7814 ± 0.0115\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7758 ± 0.0198\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65b50864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6830 ± 0.0213\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7172 ± 0.0199\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7619 ± 0.0098\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7640 ± 0.0114\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2083fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
