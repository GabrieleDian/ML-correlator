{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb101a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example workflow demonstrating how to use the improved GraphBuilder\n",
    "for planar graph classification with 11-14 nodes\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GINConv, GATConv, GCNConv, GraphConv, \n",
    "    global_mean_pool, global_max_pool, global_add_pool,\n",
    "    SAGPooling, TopKPooling, EdgePooling\n",
    ")\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, softmax\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Isomorphism Network - More powerful than GCN for graph-level tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=2, dropout=0.2, num_layers=3):\n",
    "        super(GINNet, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Build GIN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                mlp = nn.Sequential(\n",
    "                    nn.Linear(num_features, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            else:\n",
    "                mlp = nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            \n",
    "            conv = GINConv(mlp)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Jumping knowledge - combine features from all layers\n",
    "        self.jump = nn.Linear(num_layers * hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Store representations from each layer\n",
    "        layer_representations = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            \n",
    "            # Store layer representation\n",
    "            layer_representations.append(x)\n",
    "        \n",
    "        # Jumping knowledge connection\n",
    "        x = torch.cat(layer_representations, dim=-1)\n",
    "        x = self.jump(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = global_add_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cec39f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(graphs_data: List[Tuple[List, int]], \n",
    "                        feature_config) -> List[Data]:\n",
    "    \"\"\"\n",
    "    Create dataset from graph data using improved GraphBuilder\n",
    "    \n",
    "    Args:\n",
    "        graphs_data: List of (edges, label) tuples\n",
    "        feature_config: Configuration for GraphBuilder\n",
    "        \n",
    "    Returns:\n",
    "        List of PyTorch Geometric Data objects\n",
    "    \"\"\"\n",
    "    from GraphBuilder_with_features_Claude import GraphBuilder\n",
    "    \n",
    "    dataset = []\n",
    "    all_features = []\n",
    "    \n",
    "    # First pass: collect all features for normalization\n",
    "    print(\"Extracting features...\")\n",
    "    for edges, label in graphs_data:\n",
    "        builder = GraphBuilder(\n",
    "            solid_edges=edges,\n",
    "            coeff=label,\n",
    "            **feature_config\n",
    "        )\n",
    "        data = builder.build()\n",
    "        dataset.append(data)\n",
    "        all_features.append(data.x.numpy())\n",
    "    \n",
    "    # Compute normalization statistics\n",
    "    all_features = np.vstack(all_features)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_features)\n",
    "    \n",
    "    # Second pass: normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    for i, data in enumerate(dataset):\n",
    "        # Get the starting index for this graph's features\n",
    "        start_idx = sum(d.num_nodes for d in dataset[:i])\n",
    "        end_idx = start_idx + data.num_nodes\n",
    "        \n",
    "        # Normalize\n",
    "        normalized_features = scaler.transform(data.x.numpy())\n",
    "        data.x = torch.FloatTensor(normalized_features)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset)} graphs\")\n",
    "    print(f\"Feature dimensions: {dataset[0].x.shape[1]}\")\n",
    "    print(f\"Feature names: {dataset[0].feature_names}\")\n",
    "    \n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd26673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(graphs_data: List[Tuple[List, int]], \n",
    "                        feature_config: Dict) -> List[Data]:\n",
    "    \"\"\"\n",
    "    Create dataset from graph data using improved GraphBuilder\n",
    "    \n",
    "    Args:\n",
    "        graphs_data: List of (edges, label) tuples\n",
    "        feature_config: Configuration for GraphBuilder\n",
    "        \n",
    "    Returns:\n",
    "        List of PyTorch Geometric Data objects\n",
    "    \"\"\"\n",
    "    from GraphBuilder_with_features_Claude import GraphBuilder\n",
    "    \n",
    "    dataset = []\n",
    "    all_features = []\n",
    "    \n",
    "    # First pass: collect all features for normalization\n",
    "    print(\"Extracting features...\")\n",
    "    for edges, label in graphs_data:\n",
    "        builder = GraphBuilder(\n",
    "            solid_edges=edges,\n",
    "            coeff=label,\n",
    "            **feature_config\n",
    "        )\n",
    "        data = builder.build()\n",
    "        dataset.append(data)\n",
    "        all_features.append(data.x.numpy())\n",
    "    \n",
    "    # Compute normalization statistics\n",
    "    all_features = np.vstack(all_features)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_features)\n",
    "    \n",
    "    # Second pass: normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    for i, data in enumerate(dataset):\n",
    "        # Get the starting index for this graph's features\n",
    "        start_idx = sum(d.num_nodes for d in dataset[:i])\n",
    "        end_idx = start_idx + data.num_nodes\n",
    "        \n",
    "        # Normalize\n",
    "        normalized_features = scaler.transform(data.x.numpy())\n",
    "        data.x = torch.FloatTensor(normalized_features)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset)} graphs\")\n",
    "    print(f\"Feature dimensions: {dataset[0].x.shape[1]}\")\n",
    "    print(f\"Feature names: {dataset[0].feature_names}\")\n",
    "    \n",
    "    return dataset, scaler\n",
    "\n",
    "\n",
    "def evaluate_feature_importance(model, dataset, feature_names):\n",
    "    \"\"\"Evaluate which features are most important for the model\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    feature_gradients = []\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        for data in dataset:\n",
    "            data.x.requires_grad = True\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = F.nll_loss(out, data.y.unsqueeze(0))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Store gradients\n",
    "            grad = data.x.grad.abs().mean(dim=0)\n",
    "            feature_gradients.append(grad.numpy())\n",
    "    \n",
    "    # Average gradients across all graphs\n",
    "    avg_gradients = np.mean(feature_gradients, axis=0)\n",
    "    \n",
    "    # Sort features by importance\n",
    "    importance_scores = list(zip(feature_names, avg_gradients))\n",
    "    importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "def train_and_evaluate(dataset, feature_config, num_epochs=100, k_folds=5):\n",
    "    \"\"\"Train and evaluate model using k-fold cross validation with learning rate scheduling\"\"\"\n",
    "    \n",
    "    # Prepare for k-fold cross validation\n",
    "    labels = [data.y.item() for data in dataset]\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_feature_importance = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset)), labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset = [dataset[i] for i in train_idx]\n",
    "        test_dataset = [dataset[i] for i in test_idx]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        num_features = dataset[0].x.shape[1]\n",
    "        model = GINNet(num_features, hidden_dim=32, num_classes=2)\n",
    "        \n",
    "        # Initialize optimizer with a higher initial learning rate\n",
    "        # The scheduler will handle the reduction\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        # ReduceLROnPlateau: Reduces LR when validation loss plateaus\n",
    "        #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #    optimizer, \n",
    "        #    mode='min',  # Minimize loss\n",
    "        #    factor=0.5,  # Reduce LR by half\n",
    "        #    patience=10,  # Wait 10 epochs before reducing\n",
    "        #    min_lr=1e-5,  # Minimum LR\n",
    "        #)\n",
    "        \n",
    "        # Alternative schedulers you could use:\n",
    "        # 1. CosineAnnealingLR - smooth cosine decay\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        #     optimizer, T_max=num_epochs, eta_min=1e-5\n",
    "        # )\n",
    "        \n",
    "        # 2. StepLR - step decay at fixed intervals\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        #     optimizer, step_size=30, gamma=0.5\n",
    "        # )\n",
    "        \n",
    "        # 3. OneCycleLR - one cycle policy (very effective for small datasets)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=0.1, epochs=num_epochs, \n",
    "            steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        early_stop_patience = 20\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                loss = F.nll_loss(out, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Use train dataset as validation since we have limited data\n",
    "                for batch in train_loader:\n",
    "                    out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                    val_loss += F.nll_loss(out, batch.y).item()\n",
    "                    pred = out.argmax(dim=1)\n",
    "                    correct += pred.eq(batch.y).sum().item()\n",
    "                    total += batch.y.size(0)\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(train_loader)\n",
    "            val_acc = correct / total\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # For OneCycleLR, call scheduler.step() after each batch instead\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"  Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Final evaluation on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += pred.eq(batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        fold_accuracies.append(accuracy)\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Feature importance for this fold\n",
    "        if hasattr(dataset[0], 'feature_names'):\n",
    "            importance = evaluate_feature_importance(model, test_dataset, \n",
    "                                                   dataset[0].feature_names)\n",
    "            fold_feature_importance.append(importance)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cross-validation Results:\")\n",
    "    print(f\"  Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    print(f\"  All Folds: {fold_accuracies}\")\n",
    "    \n",
    "    # Average feature importance across folds\n",
    "    if fold_feature_importance:\n",
    "        avg_importance = {}\n",
    "        for feat_name in dataset[0].feature_names:\n",
    "            scores = [dict(fold)[feat_name] for fold in fold_feature_importance]\n",
    "            avg_importance[feat_name] = np.mean(scores)\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        sorted_importance = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (feat, score) in enumerate(sorted_importance[:10]):\n",
    "            print(f\"  {i+1}. {feat:25s}: {score:.4f}\")\n",
    "    \n",
    "    return fold_accuracies, avg_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ac6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_feature_sets(graphs_data):\n",
    "    \"\"\"Compare different feature configurations\"\"\"\n",
    "    \n",
    "    feature_configs = {\n",
    "        'Minimal': {\n",
    "            'selected_features': ['basic', 'face'],\n",
    "            'laplacian_pe_k': 0\n",
    "        },\n",
    "        'Planar-focused': {\n",
    "            'selected_features': ['basic', 'face', 'dual'],\n",
    "            'laplacian_pe_k': 2\n",
    "        },\n",
    "        'Balanced': {\n",
    "            'selected_features': ['basic', 'face', 'spectral_node', 'centrality'],\n",
    "            'laplacian_pe_k': 3\n",
    "        },\n",
    "        'Full': {\n",
    "            'selected_features': ['basic', 'face', 'spectral_node', 'dual', 'centrality'],\n",
    "            'laplacian_pe_k': 4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config_name, config in feature_configs.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing configuration: {config_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create dataset with this configuration\n",
    "        dataset, scaler = create_graph_dataset(graphs_data, config)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        accuracies, importance = train_and_evaluate(dataset, config, num_epochs=50)\n",
    "        \n",
    "        results[config_name] = {\n",
    "            'accuracies': accuracies,\n",
    "            'mean_accuracy': np.mean(accuracies),\n",
    "            'std_accuracy': np.std(accuracies),\n",
    "            'num_features': dataset[0].x.shape[1],\n",
    "            'importance': importance\n",
    "        }\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    configs = list(results.keys())\n",
    "    means = [results[c]['mean_accuracy'] for c in configs]\n",
    "    stds = [results[c]['std_accuracy'] for c in configs]\n",
    "    num_features = [results[c]['num_features'] for c in configs]\n",
    "    \n",
    "    x = np.arange(len(configs))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(x, means, yerr=stds, capsize=10)\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance by Feature Configuration')\n",
    "    plt.xticks(x, configs, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(num_features, means, s=100)\n",
    "    plt.errorbar(num_features, means, yerr=stds, fmt='none', capsize=5)\n",
    "    for i, config in enumerate(configs):\n",
    "        plt.annotate(config, (num_features[i], means[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Feature Dimensionality')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5caecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop=8\n",
    "# Create the edge and y lists from the csv files\\\n",
    "edges=[]\n",
    "y=[]\n",
    "for i in range(loop,loop+1):\n",
    "    filename = f'../Graph_Edge_Data/den_graph_data_{loop}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    edges += df['EDGES'].tolist()\n",
    "    y += df['COEFFICIENTS'].tolist()\n",
    "edges = [ast.literal_eval(e) for e in edges]    \n",
    "graphs_data = list(zip(edges, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "502fe948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing configuration: Minimal\n",
      "============================================================\n",
      "Extracting features...\n",
      "Normalizing features...\n",
      "Created dataset with 1432 graphs\n",
      "Feature dimensions: 5\n",
      "Feature names: ['degree', 'num_faces', 'avg_face_size', 'max_face_size', 'face_size_variance']\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/Documents/GitHub/ML-correlator/.venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PlanarGraphGNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compare different feature configurations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mcompare_feature_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mcompare_feature_sets\u001b[39m\u001b[34m(graphs_data)\u001b[39m\n\u001b[32m     31\u001b[39m     dataset, scaler = create_graph_dataset(graphs_data, config)\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     accuracies, importance = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     results[config_name] = {\n\u001b[32m     37\u001b[39m         \u001b[33m'\u001b[39m\u001b[33maccuracies\u001b[39m\u001b[33m'\u001b[39m: accuracies,\n\u001b[32m     38\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmean_accuracy\u001b[39m\u001b[33m'\u001b[39m: np.mean(accuracies),\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: importance\n\u001b[32m     42\u001b[39m     }\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Plot comparison\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(dataset, feature_config, num_epochs, k_folds)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m    105\u001b[39m num_features = dataset[\u001b[32m0\u001b[39m].x.shape[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m model = \u001b[43mPlanarGraphGNN\u001b[49m(num_features, hidden_dim=\u001b[32m32\u001b[39m, num_classes=\u001b[32m2\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Initialize optimizer with a higher initial learning rate\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# The scheduler will handle the reduction\u001b[39;00m\n\u001b[32m    110\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.1\u001b[39m, weight_decay=\u001b[32m5e-4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'PlanarGraphGNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare different feature configurations\n",
    "results = compare_feature_sets(graphs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8cca9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6627 ± 0.0105\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7081 ± 0.0319\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7388 ± 0.0286\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7681 ± 0.0143\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ba914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6836 ± 0.0247\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7479 ± 0.0072\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7814 ± 0.0115\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7758 ± 0.0198\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65b50864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: Feature Configuration Comparison\n",
      "============================================================\n",
      "\n",
      "Minimal:\n",
      "  Features: 5\n",
      "  Accuracy: 0.6830 ± 0.0213\n",
      "\n",
      "Planar-focused:\n",
      "  Features: 10\n",
      "  Accuracy: 0.7172 ± 0.0199\n",
      "\n",
      "Balanced:\n",
      "  Features: 13\n",
      "  Accuracy: 0.7619 ± 0.0098\n",
      "\n",
      "Full:\n",
      "  Features: 18\n",
      "  Accuracy: 0.7640 ± 0.0114\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Feature Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config, res in results.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Features: {res['num_features']}\")\n",
    "    print(f\"  Accuracy: {res['mean_accuracy']:.4f} ± {res['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2083fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
