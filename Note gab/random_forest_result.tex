\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{caption}

\title{Predicting Vanishing Coefficients of $f$-Graphs with Machine Learning}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\section{Setup and Motivation}
	
	We consider the set of graph called $f$-graphs contributing to the four-point correlator at fixed loop order in planar $\mathcal{N}=4$ supersymmetric Yang–Mills. Each f-graph is a has two types of edges which are graphically represented with solid and dashed lines. F-graphs are graphs with the following properties
	%
	\begin{itemize}
		\item The solid lines form a planar graph.
		\item The solid lines cannot have multiple edges connecting a pair of vertices.
		\item For each vertex the number of solid edges minus the number of dashed edges is equal to 4.
	\end{itemize}
	%	
	The $l$-loop correlator is given by a linear combination with rational coefficients of f-graphs with $4+l$ points. For example the 4-loop correlator is given by
	%
	\begin{center}
		\includegraphics[scale=1.2]{4loop.pdf}
	\end{center}
	%
	where the sum over all possible labellings is understood.
    The space of f-graphs at 1,2 and 3 loop is one-dimensional. From 8 points onward, the space becomes non-trivial. At 6-loop the first zero-coefficient f-graphs appear. By 9-loop, zero-coefficient graphs form the majority, and this proportion grows with the number of points.
	Our goal is to train machine learning models capable of predicting whether the coefficient associated with a given $f$-graph is zero or non-zero. 
	
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|r|r|r|}
		\hline
		\multicolumn{4}{|c|}{\textbf{f-graph}} \\
		\hline
		\textbf{loop} & \textbf{Total} & \textbf{\#1} & \textbf{\#0} \\
		\hline
		7  & 220     & 127    & 93 \\
		8  & 2,709   & 1,060  & 1,649 \\
		9  & 43,017  & 10,525 & 32,492 \\
		10 & 153,252 & 51,263 & 101,989 \\
		\hline
	\end{tabular}
	\caption{Summary of f-graph data across loops.}
	\label{tab:fgraph}
\end{table}
\vspace{0.5cm}


	
\section{Random forest for f-graphs}


	
	For each $f$-graph, we construct two main sets of features:
	
	\begin{itemize}
		\item \textbf{Solid line spectrum:} We compute the eigenvalue spectrum of the adjacency matrix associated with the subgraph defined by the solid lines. This captures global structural information about the propagator topology.
		
		\item \textbf{Difference spectrum:} Since different $f$-graphs can share the same denominator graph but still have different coefficients, we introduce a second matrix: the difference between the adjacency matrices of the solid line and the dashed lines. We compute the spectrum of this matrix as well. 
	\end{itemize}
	

	
		Our classification target is the column \texttt{COEFFICIENT\_DEN}, which takes binary values: $1$ if the coefficient is non-zero, and $0$ otherwise.
		
		We evaluate each model using an 80/20 train-test split. That is, 80\% of the full dataset is used for training, and the remaining 20\% is reserved for testing the model’s ability to classify unseen examples.
		
		We also perform initial experiments on the full dataset, as our goal is to determine whether a perfect classifier exists. In such a context, overfitting is not a concern: we are not interested in generalization to unseen examples, but in whether a model can in principle learn the structure of the data. 
	
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Loop}  & \textbf{Accuracy } & \textbf{Class 0 (Prec/Rec)} & \textbf{Class 1 (Prec/Rec)} & \textbf{Perfect Class.} \\
		\hline
		7  & 0.89 & 0.89 / 0.84 & 0.88 / 0.92 & yes \\
		8  & 0.83 & 0.88 / 0.85 & 0.77 / 0.82 & yes \\
		9  & 0.87 & 0.89 / 0.94 & 0.78 / 0.65 & yes \\
		10  & 0.89 & 0.91 / 0.97 & 0.76 / 0.45 & no \\
		\hline
	\end{tabular}
	\caption{Random Forest results for loops 7 to 10 using 0.8 training set and corresponding full set accuracies.}
	\label{tab:rf_loops_summary}
\end{table}

At 10-loop a perfect classification is not possible. 	The misclassified graphs indices are:
[4724, 7075, 19810, 281029, 290977, 291224, 291226, 672010, 824379]

There are 3308 cospectral graphs. 4 out of 9 of the mismatches comes from the co-spectral graphs that have different coefficients, there are
$$(4656, 4724), (290977, 290994), (672010, 672028), (280495, 281029)$$

\section{Solid graph classification}

	We now focus exclusively on the structure of the denominator graph. Rather than predicting the coefficient of each individual $f$-graph, we group all $f$-graphs that share the same denominator and ask a coarser question: \emph{does at least one $f$-graph in the family contribute to the correlator?} 


	We assign a binary label to each denominator graph:
	\begin{itemize}
		\item \textbf{1} if at least one $f$-graph with this denominator has a non-zero coefficient,
		\item \textbf{0} if all $f$-graphs with this denominator have vanishing coefficients.
	\end{itemize}
	
	This shifts the focus from fine-grained coefficient prediction to identifying which denominator topologies are relevant for the correlator.
	
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|r|r|r|}
		\textbf{Loop} & \textbf{Total} & \textbf{\#1} & \textbf{\#0} \\
		\hline
		7  & 164     & 107    & 57 \\
		8  & 1,432   & 729    & 703 \\
		9  & 13,972  & 5,661  & 8,311 \\
		10 & 900,146 & 136,433 & 763,712 \\
		\hline
	\end{tabular}
	\caption{Summary of solid graph data across loops.}
	\label{tab:solidgraph}
\end{table}

	We trained a Random Forest classifier using only the spectral features of the denominator graph, and evaluated performance across three loop orders: 8, 9, and 10 loops. For each loop order, we report results using an 80/20 train-test split. For the 9-loop case, we also tested on the full dataset to assess whether perfect classification is possible.
	
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Loop Order} & \textbf{Accuracy (80/20 split)} & \textbf{Macro F1-score} & \textbf{Accuracy (full)} \\
			\hline
			7 & 0.7879 & 0.74 &  1 \\
			8 & 0.7770 & 0.78 & 1 \\
			9 & 0.7635 & 0.75 & 0.9998 \\
			10 & 0.7959 & 0.76 & 0.99986 \\
			\hline
		\end{tabular}
		\caption{Random Forest classification performance using only denominator graph features. The macro F1-score averages performance across the two classes (0 and 1).}
		\label{tab:denominator_summary}
	\end{table}
	

	
	
	
\end{document}
	
