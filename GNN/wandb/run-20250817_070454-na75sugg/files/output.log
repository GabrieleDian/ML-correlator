Loaded config structure: ['data', 'features', 'model', 'training', 'experiment']
Sweep parameters: {'config': 'configs/config.yaml', 'learning_rate': 0.000222829373837502, 'weight_decay': 2.0839805863421456e-05}
Training on loop orders [7, 8, 9], testing on loop orders 10 with features: ['identity_columns']
Loading features: ['identity_columns']
Loaded identity_columns: shape (164, 11, 11)
Loading graph structures for loop order 7...
Loaded identity_columns: shape (1432, 12, 12)
Loading graph structures for loop order 8...
Loaded identity_columns: shape (13972, 13, 13)
Loading graph structures for loop order 9...
Created dataset with 15568 graphs
Feature dimension: 11
Standardizing to 13 features
Normalizing features...
Loading features: ['identity_columns']
Loaded identity_columns: shape (153252, 14, 14)
Loading graph structures for loop order 10...
Created dataset with 153252 graphs
Feature dimension: 14
Standardizing to 13 features
Normalizing features...

Dataset Statistics:
  Number of graphs: 15568
  Average nodes: 12.9 (min: 11, max: 13)
  Average edges: 29.8
  Label distribution: 6497 positive, 9071 negative
  Feature dimension: 13

Training configuration:
  Model: gin
  Input features: 13
  Hidden channels: 64
  Epochs: 200

Starting training...
Using device: cpu
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'train_7_8_9_test_10_loop_all_adj' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

Starting training...
Model architecture: gin
Hidden dim: 64, Layers: 3
Initial LR: 2.673952486050024e-05
Epoch   0/200: Train Loss=0.7044, Acc=0.5547, Test Loss=0.6459, Acc=0.6515, LR=0.000027
Epoch  10/200: Train Loss=0.6084, Acc=0.6599, Test Loss=0.5791, Acc=0.6821, LR=0.000079
Epoch  20/200: Train Loss=0.5319, Acc=0.7172, Test Loss=0.5340, Acc=0.7150, LR=0.000202
Epoch  30/200: Train Loss=0.5015, Acc=0.7388, Test Loss=0.5371, Acc=0.7164, LR=0.000364
Epoch  40/200: Train Loss=0.4768, Acc=0.7575, Test Loss=0.5131, Acc=0.7335, LR=0.000522
Epoch  50/200: Train Loss=0.4636, Acc=0.7651, Test Loss=0.5406, Acc=0.7224, LR=0.000634
Epoch  60/200: Train Loss=0.4426, Acc=0.7770, Test Loss=0.5290, Acc=0.7364, LR=0.000668
Epoch  70/200: Train Loss=0.4289, Acc=0.7871, Test Loss=0.5503, Acc=0.7330, LR=0.000658
Epoch  80/200: Train Loss=0.4175, Acc=0.7935, Test Loss=0.5419, Acc=0.7380, LR=0.000632
Epoch  90/200: Train Loss=0.3988, Acc=0.8045, Test Loss=0.5651, Acc=0.7220, LR=0.000591
Epoch 100/200: Train Loss=0.3908, Acc=0.8108, Test Loss=0.5645, Acc=0.7304, LR=0.000537
Epoch 110/200: Train Loss=0.3738, Acc=0.8207, Test Loss=0.6221, Acc=0.7214, LR=0.000472
Epoch 120/200: Train Loss=0.3626, Acc=0.8287, Test Loss=0.5958, Acc=0.7225, LR=0.000401
Epoch 130/200: Train Loss=0.3496, Acc=0.8353, Test Loss=0.6123, Acc=0.7317, LR=0.000327
Epoch 140/200: Train Loss=0.3350, Acc=0.8440, Test Loss=0.6926, Acc=0.7076, LR=0.000253
Epoch 150/200: Train Loss=0.3253, Acc=0.8523, Test Loss=0.6592, Acc=0.7197, LR=0.000182
Epoch 160/200: Train Loss=0.3166, Acc=0.8558, Test Loss=0.6825, Acc=0.7190, LR=0.000120
Epoch 170/200: Train Loss=0.3143, Acc=0.8568, Test Loss=0.6673, Acc=0.7211, LR=0.000068
Epoch 180/200: Train Loss=0.2991, Acc=0.8643, Test Loss=0.7235, Acc=0.7132, LR=0.000030
Epoch 190/200: Train Loss=0.3019, Acc=0.8636, Test Loss=0.6866, Acc=0.7256, LR=0.000007
Epoch 199/200: Train Loss=0.3009, Acc=0.8625, Test Loss=0.6879, Acc=0.7217, LR=0.000000

Best test accuracy: 0.7410 at epoch 58
