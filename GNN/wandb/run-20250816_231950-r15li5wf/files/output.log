Loaded config structure: ['data', 'features', 'model', 'training', 'experiment']
Sweep parameters: {'config': 'configs/config.yaml', 'learning_rate': 0.00044468233231743514, 'weight_decay': 2.1534772228903633e-05}
Training on loop orders [7, 8, 9], testing on loop orders 10 with features: ['adjacency_columns', 'eigen_1', 'eigen_2', 'eigen_3', 'degree', 'betweenness', 'clustering', 'pagerank', 'closeness', 'face_count']
Loading features: ['adjacency_columns', 'eigen_1', 'eigen_2', 'eigen_3', 'degree', 'betweenness', 'clustering', 'pagerank', 'closeness', 'face_count']
Loaded adjacency_columns: shape (164, 11, 11)
Loaded eigen_1: shape (164, 11)
Loaded eigen_2: shape (164, 11)
Loaded eigen_3: shape (164, 11)
Loaded degree: shape (164, 11)
Loaded betweenness: shape (164, 11)
Loaded clustering: shape (164, 11)
Loaded pagerank: shape (164, 11)
Loaded closeness: shape (164, 11)
Loaded face_count: shape (164, 11)
Loading graph structures for loop order 7...
Loaded adjacency_columns: shape (1432, 12, 12)
Loaded eigen_1: shape (1432, 12)
Loaded eigen_2: shape (1432, 12)
Loaded eigen_3: shape (1432, 12)
Loaded degree: shape (1432, 12)
Loaded betweenness: shape (1432, 12)
Loaded clustering: shape (1432, 12)
Loaded pagerank: shape (1432, 12)
Loaded closeness: shape (1432, 12)
Loaded face_count: shape (1432, 12)
Loading graph structures for loop order 8...
Loaded adjacency_columns: shape (13972, 13, 13)
Loaded eigen_1: shape (13972, 13)
Loaded eigen_2: shape (13972, 13)
Loaded eigen_3: shape (13972, 13)
Loaded degree: shape (13972, 13)
Loaded betweenness: shape (13972, 13)
Loaded clustering: shape (13972, 13)
Loaded pagerank: shape (13972, 13)
Loaded closeness: shape (13972, 13)
Loaded face_count: shape (13972, 13)
Loading graph structures for loop order 9...
Created dataset with 15568 graphs
Feature dimension: 20
Standardizing to 22 features
Normalizing features...
Loading features: ['adjacency_columns', 'eigen_1', 'eigen_2', 'eigen_3', 'degree', 'betweenness', 'clustering', 'pagerank', 'closeness', 'face_count']
Loaded adjacency_columns: shape (153252, 14, 14)
Loaded eigen_1: shape (153252, 14)
Loaded eigen_2: shape (153252, 14)
Loaded eigen_3: shape (153252, 14)
Loaded degree: shape (153252, 14)
Loaded betweenness: shape (153252, 14)
Loaded clustering: shape (153252, 14)
Loaded pagerank: shape (153252, 14)
Loaded closeness: shape (153252, 14)
Loaded face_count: shape (153252, 14)
Loading graph structures for loop order 10...
Created dataset with 153252 graphs
Feature dimension: 23
Standardizing to 22 features
Normalizing features...

Dataset Statistics:
  Number of graphs: 15568
  Average nodes: 12.9 (min: 11, max: 13)
  Average edges: 29.8
  Label distribution: 6497 positive, 9071 negative
  Feature dimension: 22

Training configuration:
  Model: gin
  Input features: 22
  Hidden channels: 64
  Epochs: 200

Starting training...
Using device: cpu
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'train_7_8_9_test_10_loop_all_adj' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

Starting training...
Model architecture: gin
Hidden dim: 64, Layers: 3
Initial LR: 5.3361879878092294e-05
Epoch   0/200: Train Loss=0.7073, Acc=0.5790, Test Loss=0.6334, Acc=0.6654, LR=0.000054
Epoch  10/200: Train Loss=0.5165, Acc=0.7218, Test Loss=1.8530, Acc=0.6616, LR=0.000157
Epoch  20/200: Train Loss=0.4768, Acc=0.7493, Test Loss=1.8208, Acc=0.6397, LR=0.000403
Epoch  30/200: Train Loss=0.4471, Acc=0.7675, Test Loss=1.9169, Acc=0.6034, LR=0.000727
Epoch  40/200: Train Loss=0.4210, Acc=0.7855, Test Loss=2.0972, Acc=0.5959, LR=0.001043
Epoch  50/200: Train Loss=0.3994, Acc=0.8037, Test Loss=2.3766, Acc=0.5809, LR=0.001264
Epoch  60/200: Train Loss=0.3726, Acc=0.8187, Test Loss=2.5459, Acc=0.5683, LR=0.001334
Epoch  70/200: Train Loss=0.3493, Acc=0.8320, Test Loss=2.3268, Acc=0.5591, LR=0.001314
Epoch  80/200: Train Loss=0.3262, Acc=0.8454, Test Loss=2.8982, Acc=0.5685, LR=0.001261
Epoch  90/200: Train Loss=0.2981, Acc=0.8605, Test Loss=3.6616, Acc=0.5787, LR=0.001179
Epoch 100/200: Train Loss=0.2811, Acc=0.8702, Test Loss=2.3894, Acc=0.5691, LR=0.001071
Epoch 110/200: Train Loss=0.2635, Acc=0.8801, Test Loss=2.5583, Acc=0.5523, LR=0.000943
Epoch 120/200: Train Loss=0.2361, Acc=0.8898, Test Loss=3.2441, Acc=0.5695, LR=0.000801
Epoch 130/200: Train Loss=0.2239, Acc=0.9026, Test Loss=2.7467, Acc=0.5487, LR=0.000652
Epoch 140/200: Train Loss=0.2120, Acc=0.9053, Test Loss=3.1599, Acc=0.5645, LR=0.000504
Epoch 150/200: Train Loss=0.1967, Acc=0.9173, Test Loss=3.6495, Acc=0.5639, LR=0.000364
Epoch 160/200: Train Loss=0.1788, Acc=0.9225, Test Loss=4.1872, Acc=0.5651, LR=0.000240
Epoch 170/200: Train Loss=0.1680, Acc=0.9296, Test Loss=4.3755, Acc=0.5652, LR=0.000136
Epoch 180/200: Train Loss=0.1604, Acc=0.9289, Test Loss=4.2409, Acc=0.5645, LR=0.000060
Epoch 190/200: Train Loss=0.1540, Acc=0.9341, Test Loss=4.1191, Acc=0.5680, LR=0.000014
Epoch 199/200: Train Loss=0.1547, Acc=0.9362, Test Loss=4.3611, Acc=0.5683, LR=0.000000

Best test accuracy: 0.6654 at epoch 0
