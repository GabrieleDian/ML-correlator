Loaded config structure: ['data', 'features', 'model', 'training', 'experiment']
Sweep parameters: {'config': 'configs/config.yaml', 'learning_rate': 0.0002404332617416531, 'weight_decay': 1.819015203028599e-05}
Training on loop orders [7, 8, 9], testing on loop orders 10 with features: ['identity_columns']
Loading features: ['identity_columns']
Loaded identity_columns: shape (164, 11, 11)
Loading graph structures for loop order 7...
Loaded identity_columns: shape (1432, 12, 12)
Loading graph structures for loop order 8...
Loaded identity_columns: shape (13972, 13, 13)
Loading graph structures for loop order 9...
Created dataset with 15568 graphs
Feature dimension: 11
Standardizing to 13 features
Normalizing features...
Loading features: ['identity_columns']
Loaded identity_columns: shape (153252, 14, 14)
Loading graph structures for loop order 10...
Created dataset with 153252 graphs
Feature dimension: 14
Standardizing to 13 features
Normalizing features...

Dataset Statistics:
  Number of graphs: 15568
  Average nodes: 12.9 (min: 11, max: 13)
  Average edges: 29.8
  Label distribution: 6497 positive, 9071 negative
  Feature dimension: 13

Training configuration:
  Model: gin
  Input features: 13
  Hidden channels: 64
  Epochs: 200

Starting training...
Using device: cpu
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'train_7_8_9_test_10_loop_all_adj' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

Starting training...
Model architecture: gin
Hidden dim: 64, Layers: 3
Initial LR: 2.8851991408998393e-05
Epoch   0/200: Train Loss=0.7026, Acc=0.5565, Test Loss=0.6447, Acc=0.6543, LR=0.000029
Epoch  10/200: Train Loss=0.6032, Acc=0.6658, Test Loss=0.5731, Acc=0.6861, LR=0.000085
Epoch  20/200: Train Loss=0.5299, Acc=0.7183, Test Loss=0.5275, Acc=0.7172, LR=0.000218
Epoch  30/200: Train Loss=0.5008, Acc=0.7413, Test Loss=0.5322, Acc=0.7171, LR=0.000393
Epoch  40/200: Train Loss=0.4751, Acc=0.7603, Test Loss=0.5100, Acc=0.7309, LR=0.000564
Epoch  50/200: Train Loss=0.4630, Acc=0.7663, Test Loss=0.5355, Acc=0.7223, LR=0.000684
Epoch  60/200: Train Loss=0.4411, Acc=0.7788, Test Loss=0.5171, Acc=0.7366, LR=0.000721
Epoch  70/200: Train Loss=0.4309, Acc=0.7874, Test Loss=0.5389, Acc=0.7338, LR=0.000710
Epoch  80/200: Train Loss=0.4144, Acc=0.7967, Test Loss=0.5361, Acc=0.7414, LR=0.000682
Epoch  90/200: Train Loss=0.3974, Acc=0.8094, Test Loss=0.5435, Acc=0.7255, LR=0.000637
Epoch 100/200: Train Loss=0.3904, Acc=0.8138, Test Loss=0.5677, Acc=0.7331, LR=0.000579
Epoch 110/200: Train Loss=0.3698, Acc=0.8246, Test Loss=0.5946, Acc=0.7276, LR=0.000510
Epoch 120/200: Train Loss=0.3555, Acc=0.8349, Test Loss=0.6048, Acc=0.7146, LR=0.000433
Epoch 130/200: Train Loss=0.3477, Acc=0.8329, Test Loss=0.6053, Acc=0.7219, LR=0.000353
Epoch 140/200: Train Loss=0.3361, Acc=0.8440, Test Loss=0.6537, Acc=0.7156, LR=0.000273
Epoch 150/200: Train Loss=0.3251, Acc=0.8480, Test Loss=0.6326, Acc=0.7243, LR=0.000197
Epoch 160/200: Train Loss=0.3161, Acc=0.8539, Test Loss=0.6538, Acc=0.7209, LR=0.000130
Epoch 170/200: Train Loss=0.3126, Acc=0.8575, Test Loss=0.6535, Acc=0.7239, LR=0.000074
Epoch 180/200: Train Loss=0.2970, Acc=0.8655, Test Loss=0.7307, Acc=0.7134, LR=0.000032
Epoch 190/200: Train Loss=0.2984, Acc=0.8622, Test Loss=0.6770, Acc=0.7290, LR=0.000007
Epoch 199/200: Train Loss=0.2991, Acc=0.8636, Test Loss=0.6795, Acc=0.7266, LR=0.000000

Best test accuracy: 0.7434 at epoch 58
