Loaded config structure: ['data', 'features', 'model', 'training', 'experiment']
Sweep parameters: {'config': 'configs/config.yaml', 'learning_rate': 0.00024759880248501645, 'weight_decay': 3.502825752792683e-05}
Training on loop orders [7, 8, 9], testing on loop orders 10 with features: ['identity_columns']
Loading features: ['identity_columns']
Loaded identity_columns: shape (164, 11, 11)
Loading graph structures for loop order 7...
Loaded identity_columns: shape (1432, 12, 12)
Loading graph structures for loop order 8...
Loaded identity_columns: shape (13972, 13, 13)
Loading graph structures for loop order 9...
Created dataset with 15568 graphs
Feature dimension: 11
Standardizing to 13 features
Normalizing features...
Loading features: ['identity_columns']
Loaded identity_columns: shape (153252, 14, 14)
Loading graph structures for loop order 10...
Created dataset with 153252 graphs
Feature dimension: 14
Standardizing to 13 features
Normalizing features...

Dataset Statistics:
  Number of graphs: 15568
  Average nodes: 12.9 (min: 11, max: 13)
  Average edges: 29.8
  Label distribution: 6497 positive, 9071 negative
  Feature dimension: 13

Training configuration:
  Model: gin
  Input features: 13
  Hidden channels: 64
  Epochs: 200

Starting training...
Using device: cpu
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'train_7_8_9_test_10_loop_all_adj' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

Starting training...
Model architecture: gin
Hidden dim: 64, Layers: 3
Initial LR: 2.9711856298202024e-05
Epoch   0/200: Train Loss=0.7019, Acc=0.5567, Test Loss=0.6444, Acc=0.6547, LR=0.000030
Epoch  10/200: Train Loss=0.6012, Acc=0.6684, Test Loss=0.5754, Acc=0.6818, LR=0.000087
Epoch  20/200: Train Loss=0.5285, Acc=0.7164, Test Loss=0.5313, Acc=0.7162, LR=0.000224
Epoch  30/200: Train Loss=0.5003, Acc=0.7454, Test Loss=0.5362, Acc=0.7077, LR=0.000405
Epoch  40/200: Train Loss=0.4729, Acc=0.7595, Test Loss=0.5025, Acc=0.7395, LR=0.000580
Epoch  50/200: Train Loss=0.4645, Acc=0.7660, Test Loss=0.5309, Acc=0.7268, LR=0.000704
Epoch  60/200: Train Loss=0.4440, Acc=0.7785, Test Loss=0.5251, Acc=0.7371, LR=0.000743
Epoch  70/200: Train Loss=0.4293, Acc=0.7905, Test Loss=0.5526, Acc=0.7268, LR=0.000732
Epoch  80/200: Train Loss=0.4115, Acc=0.7929, Test Loss=0.5621, Acc=0.7358, LR=0.000702
Epoch  90/200: Train Loss=0.3982, Acc=0.8071, Test Loss=0.5590, Acc=0.7338, LR=0.000656
Epoch 100/200: Train Loss=0.3905, Acc=0.8129, Test Loss=0.5498, Acc=0.7353, LR=0.000596
Epoch 110/200: Train Loss=0.3692, Acc=0.8228, Test Loss=0.6135, Acc=0.7166, LR=0.000525
Epoch 120/200: Train Loss=0.3566, Acc=0.8320, Test Loss=0.6088, Acc=0.7158, LR=0.000446
Epoch 130/200: Train Loss=0.3425, Acc=0.8396, Test Loss=0.5927, Acc=0.7259, LR=0.000363
Epoch 140/200: Train Loss=0.3316, Acc=0.8487, Test Loss=0.7065, Acc=0.7127, LR=0.000281
Epoch 150/200: Train Loss=0.3215, Acc=0.8536, Test Loss=0.6604, Acc=0.7235, LR=0.000203
Epoch 160/200: Train Loss=0.3096, Acc=0.8613, Test Loss=0.6574, Acc=0.7231, LR=0.000133
Epoch 170/200: Train Loss=0.3065, Acc=0.8589, Test Loss=0.6743, Acc=0.7245, LR=0.000076
Epoch 180/200: Train Loss=0.2961, Acc=0.8665, Test Loss=0.7245, Acc=0.7146, LR=0.000033
Epoch 190/200: Train Loss=0.2974, Acc=0.8649, Test Loss=0.6835, Acc=0.7289, LR=0.000008
Epoch 199/200: Train Loss=0.2935, Acc=0.8677, Test Loss=0.6845, Acc=0.7258, LR=0.000000

Best test accuracy: 0.7399 at epoch 73
