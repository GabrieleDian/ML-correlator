Loaded config structure: ['data', 'features', 'model', 'training', 'experiment']
Sweep parameters: {'config': 'configs/config.yaml', 'learning_rate': 0.00027229677435419106, 'weight_decay': 2.667913474013346e-05}
Training on loop orders [7, 8, 9], testing on loop orders 10 with features: ['identity_columns']
Loading features: ['identity_columns']
Loaded identity_columns: shape (164, 11, 11)
Loading graph structures for loop order 7...
Loaded identity_columns: shape (1432, 12, 12)
Loading graph structures for loop order 8...
Loaded identity_columns: shape (13972, 13, 13)
Loading graph structures for loop order 9...
Created dataset with 15568 graphs
Feature dimension: 11
Standardizing to 13 features
Normalizing features...
Loading features: ['identity_columns']
Loaded identity_columns: shape (153252, 14, 14)
Loading graph structures for loop order 10...
Created dataset with 153252 graphs
Feature dimension: 14
Standardizing to 13 features
Normalizing features...

Dataset Statistics:
  Number of graphs: 15568
  Average nodes: 12.9 (min: 11, max: 13)
  Average edges: 29.8
  Label distribution: 6497 positive, 9071 negative
  Feature dimension: 13

Training configuration:
  Model: gin
  Input features: 13
  Hidden channels: 64
  Epochs: 200

Starting training...
Using device: cpu
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'train_7_8_9_test_10_loop_all_adj' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.

Starting training...
Model architecture: gin
Hidden dim: 64, Layers: 3
Initial LR: 3.267561292250292e-05
Epoch   0/200: Train Loss=0.7000, Acc=0.5587, Test Loss=0.6433, Acc=0.6559, LR=0.000033
Epoch  10/200: Train Loss=0.5942, Acc=0.6759, Test Loss=0.5725, Acc=0.6820, LR=0.000096
Epoch  20/200: Train Loss=0.5249, Acc=0.7227, Test Loss=0.5308, Acc=0.7150, LR=0.000247
Epoch  30/200: Train Loss=0.4982, Acc=0.7422, Test Loss=0.5341, Acc=0.7116, LR=0.000445
Epoch  40/200: Train Loss=0.4731, Acc=0.7592, Test Loss=0.5094, Acc=0.7307, LR=0.000638
Epoch  50/200: Train Loss=0.4642, Acc=0.7679, Test Loss=0.5329, Acc=0.7281, LR=0.000774
Epoch  60/200: Train Loss=0.4396, Acc=0.7809, Test Loss=0.5203, Acc=0.7364, LR=0.000817
Epoch  70/200: Train Loss=0.4301, Acc=0.7910, Test Loss=0.5904, Acc=0.7215, LR=0.000805
Epoch  80/200: Train Loss=0.4122, Acc=0.7946, Test Loss=0.5425, Acc=0.7390, LR=0.000772
Epoch  90/200: Train Loss=0.3914, Acc=0.8097, Test Loss=0.5640, Acc=0.7166, LR=0.000722
Epoch 100/200: Train Loss=0.3809, Acc=0.8196, Test Loss=0.5719, Acc=0.7290, LR=0.000656
Epoch 110/200: Train Loss=0.3631, Acc=0.8260, Test Loss=0.6070, Acc=0.7288, LR=0.000577
Epoch 120/200: Train Loss=0.3503, Acc=0.8376, Test Loss=0.6171, Acc=0.7160, LR=0.000490
Epoch 130/200: Train Loss=0.3440, Acc=0.8412, Test Loss=0.5845, Acc=0.7275, LR=0.000399
Epoch 140/200: Train Loss=0.3249, Acc=0.8528, Test Loss=0.6447, Acc=0.7168, LR=0.000309
Epoch 150/200: Train Loss=0.3190, Acc=0.8527, Test Loss=0.6202, Acc=0.7299, LR=0.000223
Epoch 160/200: Train Loss=0.3076, Acc=0.8632, Test Loss=0.6443, Acc=0.7288, LR=0.000147
Epoch 170/200: Train Loss=0.3034, Acc=0.8627, Test Loss=0.6731, Acc=0.7279, LR=0.000083
Epoch 180/200: Train Loss=0.2896, Acc=0.8672, Test Loss=0.7108, Acc=0.7173, LR=0.000037
Epoch 190/200: Train Loss=0.2930, Acc=0.8664, Test Loss=0.6793, Acc=0.7320, LR=0.000008
Epoch 199/200: Train Loss=0.2914, Acc=0.8695, Test Loss=0.6842, Acc=0.7302, LR=0.000000

Best test accuracy: 0.7448 at epoch 52
