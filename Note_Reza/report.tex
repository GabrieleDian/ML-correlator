\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{siunitx}

\title{Predicting Vanishing Coefficients of Denominator Graphs: From $l$-loops to $(l+1)$-loops OR Zero and few shot learning}
\author{}
\date{}

\begin{document}
	
	\maketitle
	\tableofcontents
	\section{Introduction}
	
	We consider the denominator graphs. It is understood that if the denominator has zero coefficient then any full graph that uses it will also have a coefficient of zero. The denominator graph is likely to represent the easiest task we can solve.
	
	In this short document we will investigate predicting higher-loop coefficients from lower-loop coefficients. This goes towards the overall ambition of predicting coefficients of currently unseen or unknown loops.
	

\section{Tabular Features}

The notebook produces a diverse set of graph-based features, each reflecting a specific structural property of the graph. Below we describe each feature group and its potential usefulness.

\subsection*{Core-related Features}
\begin{itemize}
    \item \texttt{Core\_core\_index\_mean} --- The average \emph{core index} across all nodes. A node’s core index measures its position in the $k$-core decomposition, indicating how deeply embedded it is in the network. Useful for quantifying overall network cohesion.
    \item \texttt{Core\_max\_core\_index} --- The maximum core index found in the graph. High values suggest a highly interconnected core, indicating robustness or a central structure.
\end{itemize}

\subsection*{Cycle-based Features}
\begin{itemize}
    \item \texttt{Cycle\_num\_cycles\_len\_5}, \texttt{Cycle\_num\_cycles\_len\_6} --- Counts of cycles of length 5 and 6. Cycle counts can reveal repeating motifs in networks, important in chemistry (ring structures in molecules) or in network motif analysis.
\end{itemize}

\subsection*{Basic Connectivity Metrics}
\begin{itemize}
    \item \texttt{EDGES}, \texttt{NUM\_EDGES} --- Number of edges in the graph. A fundamental density measure.
    \item \texttt{DEN\_EDGES} --- Likely edge density (ratio of actual edges to possible edges). High density means more connections between nodes.
\end{itemize}

\subsection*{Spectral Graph Theory Features}
\begin{itemize}
    \item \texttt{Spectral\_algebraic\_connectivity} --- The second-smallest eigenvalue of the Laplacian; higher values mean the graph is harder to disconnect, useful for robustness analysis.
    \item \texttt{Spectral\_laplacian\_mean}, \texttt{Spectral\_laplacian\_std}, \texttt{Spectral\_laplacian\_skew} --- Statistical moments of the Laplacian spectrum, capturing the shape of the spectrum which reflects graph connectivity patterns.
    \item \texttt{Spectral\_spectral\_gap} --- The difference between the first two eigenvalues of the adjacency or Laplacian matrix; often relates to community structure detectability.
\end{itemize}

\subsection*{Planarity-based Features}
\begin{itemize}
    \item \texttt{Planarity\_num\_faces} --- Number of faces in a planar embedding (if planar). More faces can suggest more complex local structures.
    \item \texttt{Planarity\_face\_size\_max}, \texttt{Planarity\_face\_size\_mean} --- Maximum and average number of edges per face; reflects how ``spread out'' cycles are in a planar representation.
\end{itemize}

\subsection*{Symmetry Features}
\begin{itemize}
    \item \texttt{Symmetry\_automorphism\_group\_order} --- Size of the automorphism group (number of graph symmetries). Higher values indicate more structural regularity.
    \item \texttt{Symmetry\_num\_orbits} --- Number of distinct node equivalence classes under symmetries. Fewer orbits mean more symmetry.
    \item \texttt{Symmetry\_orbit\_size\_max} --- Largest equivalence class of nodes under symmetries; can highlight repeated patterns.
\end{itemize}

\subsection*{Robustness Features}
\begin{itemize}
    \item \texttt{Robust\_articulation\_points} --- Number of articulation points (nodes whose removal increases the number of connected components). Fewer points means more robust connectivity.
    \item \texttt{Robust\_bridge\_count} --- Number of bridges (edges whose removal disconnects the graph). Fewer bridges imply greater robustness.
\end{itemize}

\subsection*{Global Network Descriptors}
\begin{itemize}
    \item \texttt{Kirchhoff\_index} --- A resistance-based distance measure, summing effective resistances over all pairs of nodes. Useful for characterising transport efficiency or redundancy.
\end{itemize}


	
	\section{Methodology}
	In this work, our objective is to predict the properties of \emph{unseen loops} by leveraging the currently known lower-loop coefficients. We frame this task as a supervised learning problem using the \texttt{XGBoost} algorithm, which is widely recognized as one of the most competitive approaches for tabular data, often outperforming or matching the performance of deep neural networks in such settings. Importantly, this setup naturally corresponds to a \emph{zero-shot learning} scenario: at inference time, the model is required to make predictions for $(l+1)$-loop quantities without having observed any corresponding training data for that loop order. While the primary focus is on the zero-shot setting, we will also explore the impact of incorporating a small amount of $(l+1)$-loop data into training to assess potential performance gains.
	
	This setup corresponds to a zero-shot learning scenario in the strict case where no $(l+1)$-loop data are included during training (with the model generalizing solely from patterns learned at lower loops), whereas the inclusion of even a small fraction of $(l+1)$-loop examples would instead place it in a few-shot learning regime.


We will use cross validation in all our experiments with target stratification - with the average cross validated score over folds being our main result. We will use ROC-AUC to measure performance. In general - the interpretation of the ROC-AUC score is if I randomly choose a graph with coefficient non-zero and a graph with coefficient zero, what is the probability that the model output score of first graph is larger than the second.

Finally, our analysis covers loops 7,8,9 and 10 - mostly because other edge data was not available.

\section{Results and conclusions}

\subsection{Intra-loop and mixed loops}




\begin{table}[h]
\centering
\begin{tabular}{c|c|ccccc|cc}
\hline
Pair & Loop & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Avg ROC AUC & Std Dev \\
\hline
\multicolumn{9}{c}{\textbf{Single Loops}} \\
\hline
-- & 7  & 0.789 & 0.847 & 0.817 & 0.845 & 0.662 & 0.792 & 0.068 \\
-- & 8  & 0.859 & 0.865 & 0.882 & 0.879 & 0.869 & 0.871 & 0.009 \\
-- & 9  & 0.904 & 0.893 & 0.900 & 0.904 & 0.904 & 0.901 & 0.004 \\
-- & 10 & 0.914 & 0.916 & 0.914 & 0.916 & 0.918 & 0.916 & 0.001 \\
-- & 11 & 0.919 & 0.920 & 0.920 & 0.918 & 0.919 & 0.919 & 0.000 \\
\hline
\multicolumn{9}{c}{\textbf{Mixed Loops}} \\
\hline
7 \& 8  & 7  & 0.884 & 0.888 & 0.810 & 0.873 & 0.814 & 0.854 & 0.035 \\
        & 8  & 0.847 & 0.884 & 0.864 & 0.874 & 0.839 & 0.862 & 0.017 \\
8 \& 9  & 8  & 0.868 & 0.887 & 0.883 & 0.882 & 0.876 & 0.879 & 0.007 \\
        & 9  & 0.899 & 0.894 & 0.906 & 0.898 & 0.909 & 0.901 & 0.006 \\
9 \& 10 & 9  & 0.902 & 0.895 & 0.896 & 0.912 & 0.905 & 0.902 & 0.006 \\
        & 10 & 0.916 & 0.914 & 0.913 & 0.914 & 0.916 & 0.914 & 0.001 \\
10 \& 11 & 10  & 0.906 & 0.905 & 0.905 & 0.907 & 0.909 & 0.906 & 0.002 \\
        & 11 & 0.919 & 0.919 & 0.918 & 0.918 & 0.918 & 0.919 & 0.000 \\
8 \& 10 & 8  & 0.850 & 0.856 & 0.864 & 0.872 & 0.845 & 0.858 & 0.010 \\
        & 10 & 0.917 & 0.915 & 0.914 & 0.917 & 0.914 & 0.915 & 0.001 \\
7 \& 9  & 7  & 0.744 & 0.847 & 0.782 & 0.877 & 0.714 & 0.793 & 0.061 \\
        & 9  & 0.910 & 0.888 & 0.905 & 0.896 & 0.902 & 0.900 & 0.008 \\
7 \& 10 & 7  & 0.756 & 0.736 & 0.774 & 0.810 & 0.662 & 0.747 & 0.049 \\
        & 10 & 0.918 & 0.917 & 0.915 & 0.916 & 0.915 & 0.916 & 0.001 \\
\hline
\end{tabular}
\caption{ROC AUC scores for single loops and mixed loop combinations across 5 folds, with mean and standard deviation.}
\label{tab:roc_auc_all}
\end{table}

A few notables:

\begin{itemize}

\item Mixing $l+1$ with $l$ always improves performance on $l$ 
\item Mixing $l+1$ with $l$ seems to degrade performance on $l+1$
\end{itemize}
$$\forall f^{\nu^{(n-1)}}_{j,\alpha} :
\sum_{f^{\rho^{(n)}}_{i,\mu}}
\frac{\left| f^{\nu^{(n-1)}}_{j,\alpha} \right|}
     {\left| f^{\rho^{(n)}}_{i,\mu} \right|}
\, c^{(n)}_{i}
= c^{(n-1)}_{j} ;
\quad \text{in particular,} \quad
c^{(n-1)}_{0} \equiv 0 
$$

Cusp rule tells us that pinching on an $l$ loop coefficient is equivalent to the weighted sum of $l+1$ loop coefficients. This would mean that there is information in $l+1$ loops that $l$ loops can use and we indeed observe this in the results. (Note - we should do experiments or study ways to deconstruct this from simply having more data - not sure how much an issue this is).


\subsection{Directionality: Zero-shot learning}

In this section - we are modelling upwards - what we can call directionality. We train a model on lower loops in order to predict on higher loops. If we want to make use of a model to predict loops currently unseen we would want to do this.

This is an example of zero-shot learning. We will not show the model any information about higher loops - instead we want it to learn enough structure in lower loops and make good inferences of high loops based on information available.

As an exercise we will show the precision recall curves - recall that high precision means fewer false positives whilst higher recall means fewer false negatives.


\subsubsection{$[7]\rightarrow [8,9,10]$}
\begin{table}[H]
\centering
\begin{tabular}{l|c}
\hline
Dataset & ROC AUC \\
\hline
Train [7]  & 1.000 \\
Test [8]   & 0.760 \\
Test [9]   & 0.754 \\
Test [10]  & 0.699 \\
\hline
\end{tabular}
\caption{Directionality summary: trained on loop 7, evaluated on loops 8–10.}
\label{tab:directionality_summary_7}
\end{table}


\begin{figure}[H] % h = here
\centering
\includegraphics[width=1.0\textwidth]{assets/7_8910.png}
\caption{Threshold plots}
\label{fig:my_image1}
\end{figure}

Clearly - these are expectantly poor results.

\begin{itemize}
\item Recall drops off quickly indicating large amount false negatives comes in when we increase the threshold - i.e. bringing up the threshold makes the model think there are zero coefficients when there aren't (we expect this - but its happening quickly).
\item Precision never reaches 1. If we push the threshold to near 1, a well calibrated model will reveal the high confidence coefficients - we do not get that here.
\end{itemize}

\subsubsection{$[7,8]\rightarrow [9,10]$}
\begin{table}[H]
\centering
\begin{tabular}{l|c}
\hline
Dataset & ROC AUC \\
\hline
Train [7, 8]  & 1.000 \\
Test [9]      & 0.790 \\
Test [10]     & 0.799 \\
\hline
\end{tabular}
\caption{Directionality summary: trained on loops 7 and 8, evaluated on loops 9 and 10.}
\label{tab:directionality_summary_78}
\end{table}

\begin{figure}[h] % h = here
\centering
\includegraphics[width=1.0\textwidth]{assets/78_910.png}
\caption{Threshold plots}
\label{fig:my_image}
\end{figure}


\begin{itemize}
\item Recall drops off much slower than before - bringing the threshold up does not quickly drop coefficients - it is likely that the model is not well calibrated.
\item Precision never reaches 1. If we push the threshold to near 1, a well calibrated model will reveal the high confidence coefficients - we do not get that here.
\end{itemize}


\subsubsection{$[7,8,9 ]\rightarrow [10]$}
\begin{table}[H]
\centering
\begin{tabular}{l|c}
\hline
Dataset & ROC AUC \\
\hline
Train [7, 8, 9] & 0.997 \\
Test [10]       & 0.817 \\
\hline
\end{tabular}
\caption{Directionality summary: trained on loops 7, 8, and 9, evaluated on loop 10.}
\label{tab:directionality_summary_789}
\end{table}

\begin{figure}[h] % h = here
\centering
\includegraphics[width=1.0\textwidth]{assets/789_10.png}
\caption{Threshold plots}
\label{fig:my_image}
\end{figure}

This is the case that is of interest to us in this current study.

Really importantly - the training AUC-ROC does not reach 1. This indicates conflicting datapoints in loop 9 and data in loops 7 and 8. We need to enhance the feature space further to remove this conflict.

\begin{itemize}
\item Recall drops off quite linearly indicating the best calibrated we have had so far.
\item Again precision never reaches 1. If we push the threshold to near 1, a well calibrated model will reveal the high confidence coefficients - we do not get that here.
\end{itemize}

\subsubsection{$[7,8,9,10 ]\rightarrow [11]$}
\begin{table}[H]
\centering
\begin{tabular}{l|c}
\hline
Dataset & ROC AUC \\
\hline
Train [7, 8, 9,10] & 0.943 \\
Test [11]       & 0.857 \\
\hline
\end{tabular}
\caption{Directionality summary: trained on loops 7, 8, 9, adn 10 evaluated on loop 11.}
\label{tab:directionality_summary_78910}
\end{table}
\subsection{Directionality + pieces of loop 10 needed to get intra-loop 10 level performance: Few-shot learning}

In this section we look a the $[7,8,9]  \rightarrow [10]$ problem again but we now cluster the 10 loop graphs and work out which clusters have the mosts value.

This is something akin for Few-shot learning (although we are taking quite a few more 10 loop examples into a training set than we might normally).

\begin{table}[H]
\centering
\begin{tabular}{c|rrr|c}
\hline
Cluster & Rows $\rightarrow$ Train & Remaining Test Rows & AUC \\
\hline
0  & 21310 & 147510 & 0.889 \\
1  &  4927 & 163893 & 0.831 \\
2  & 12960 & 155860 & 0.864 \\
3  &  2294 & 166526 & 0.850 \\
4  &  1424 & 167396 & 0.852 \\
5  & 15156 & 153664 & \textbf{0.890} \\
6  &  2467 & 166353 & 0.849 \\
7  & 16574 & 152246 & 0.878 \\
8  & 21277 & 147543 & \textbf{0.890} \\
9  &   143 & 168677 & 0.834 \\
10 &  6077 & 162743 & 0.838 \\
11 &   654 & 168166 & 0.828 \\
12 &  2600 & 166220 & 0.871 \\
13 &  6539 & 162281 & 0.830 \\
14 &  2476 & 166344 & 0.867 \\
15 &  8740 & 160080 & 0.851 \\
16 & 16018 & 152802 & 0.872 \\
17 & 12670 & 156150 & 0.862 \\
18 &   164 & 168656 & 0.835 \\
19 & 14350 & 154470 & 0.867 \\
\hline
\end{tabular}
\caption{Per-cluster AUC results when promoting one cluster from loop 10 into training. Highest values in bold.}
\label{tab:promote_one_cluster}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{l|l|c}
\hline
Promotion Step & Clusters Promoted & Overall AUC \\
\hline
Best 1st Cluster & [5] & 0.890 \\
After 2 Clusters & [5, 8] & 0.908 \\
After 3 Clusters & [5, 8, 19] & 0.917 \\
\hline
\end{tabular}
\caption{Progressive improvement in AUC as additional clusters are promoted into the training set.}
\label{tab:multi_cluster_promotion}
\end{table}

The additional clusters bring in 50783 more datapoints which are 10-loop specific. Training with the same amount of 10-loop data on a 10-loop problem alone gives a ROC AUC of about 0.907. Ultimately - its not so clear if this is a useful thing to do until we do further comparisons.
\section{Uncertainty Quantification}
TODO

\section{Next steps}
Questions to ask ourselves in this work



% Optional global tweaks
\setlist{nosep}                       % tighter lists
\setlist[itemize]{leftmargin=*}       % full-width bullets
\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\alph*)}  % sublevel like a), b), ...

\begin{enumerate}
\item Try this on higher loops again
 \item Move to full graph
  \item Data Augmentation
    \begin{enumerate}
      \item In vanilla modelling - we expect there to be a rich generating function - not particularly obvious how invariance we have that is not already encoded in features
      \item Fix the 9-loop problem.
    \end{enumerate}
  \item Confidence scores and how in particular to use for 13-loops
   \item Are we finding coefficients for graphs that cannot be found through rules like cusp rules?
\end{enumerate}

On the GNN approach - we should get graph representations that are far richer.


\section{Tree Misclassification by inclusion of loop order}

% Preamble

\sisetup{
  detect-all,
  input-decimal-markers = {.},
  group-separator = {,}
}

% ===== Table 1: Overall by combo =====
\begin{table}[H]
\centering
\small
\caption{Overall by combo}
\label{tab:overall-by-combo-uniq}
\begin{tabular}{
  l % text column for train_loops
  S[table-format=6.0]
  S[table-format=1.6]
  S[table-format=2.0]
  S[table-format=1.6]
}
\toprule
{train\_loops} & {n\_train} & {train\_auc\_overall} & {test\_loop} & {test\_auc} \\
\midrule
(9, 10)        & 167224 & 0.942495 & 11 & 0.869956 \\
(7, 9, 10)     & 167388 & 0.943104 & 11 & 0.864540 \\
(8, 9, 10)     & 168656 & 0.942171 & 11 & 0.862627 \\
(7, 8, 9, 10)  & 168820 & 0.942916 & 11 & 0.856828 \\
(8, 10)        & 154684 & 0.945206 & 11 & 0.840198 \\
(7, 8, 10)     & 154848 & 0.945260 & 11 & 0.838728 \\
(7, 8)         &   1596 & 1.000000 & 11 & 0.811331 \\
(10,)          & 153252 & 0.945841 & 11 & 0.809871 \\
(8,)           &   1432 & 1.000000 & 11 & 0.807204 \\
(9,)           &  13972 & 0.997817 & 11 & 0.798707 \\
(7, 9)         &  14136 & 0.998211 & 11 & 0.793281 \\
(8, 9)         &  15404 & 0.996969 & 11 & 0.789976 \\
(7, 10)        & 153416 & 0.946195 & 11 & 0.786496 \\
(7, 8, 9)      &  15568 & 0.996618 & 11 & 0.779690 \\
(7,)           &    164 & 1.000000 & 11 & 0.636116 \\
\bottomrule
\end{tabular}
\end{table}

% ===== Table 2: Training per-loop AUCs =====
\begin{table}[htbp]
\centering
\small
\caption{Training per-loop AUCs (long-form)}
\label{tab:training-per-loop-uniq}
\begin{tabular}{
  l % text column for train_loops
  S[table-format=2.0]
  S[table-format=6.0]
  S[table-format=1.6]
}
\toprule
{train\_loops} & {loop\_id} & {n\_train\_loop} & {train\_auc\_loop} \\
\midrule
(7,)          &  7 &    164  & 1.000000 \\
(7, 8)        &  7 &    164  & 1.000000 \\
(7, 8)        &  8 &   1432  & 1.000000 \\
(7, 8, 9)     &  7 &    164  & 1.000000 \\
(7, 8, 9)     &  8 &   1432  & 0.999772 \\
(7, 8, 9)     &  9 &  13972  & 0.996046 \\
(7, 8, 9, 10) &  7 &    164  & 0.996557 \\
(7, 8, 9, 10) &  8 &   1432  & 0.970624 \\
(7, 8, 9, 10) &  9 &  13972  & 0.946507 \\
(7, 8, 9, 10) & 10 & 153252  & 0.942077 \\
(7, 8, 10)    &  7 &    164  & 0.986719 \\
(7, 8, 10)    &  8 &   1432  & 0.970495 \\
(7, 8, 10)    & 10 & 153252  & 0.944890 \\
(7, 9)        &  7 &    164  & 1.000000 \\
(7, 9)        &  9 &  13972  & 0.998165 \\
(7, 9, 10)    &  7 &    164  & 0.971635 \\
(7, 9, 10)    &  9 &  13972  & 0.952468 \\
(7, 9, 10)    & 10 & 153252  & 0.942129 \\
(7, 10)       &  7 &    164  & 0.999016 \\
(7, 10)       & 10 & 153252  & 0.946104 \\
(8,)          &  8 &   1432  & 1.000000 \\
(8, 9)        &  8 &   1432  & 0.999653 \\
(8, 9)        &  9 &  13972  & 0.996556 \\
(8, 9, 10)    &  8 &   1432  & 0.962762 \\
(8, 9, 10)    &  9 &  13972  & 0.944510 \\
(8, 9, 10)    & 10 & 153252  & 0.941615 \\
(8, 10)       &  8 &   1432  & 0.977820 \\
(8, 10)       & 10 & 153252  & 0.944844 \\
(9,)          &  9 &  13972  & 0.997817 \\
(9, 10)       &  9 &  13972  & 0.953727 \\
(9, 10)       & 10 & 153252  & 0.941386 \\
(10,)         & 10 & 153252  & 0.945841 \\
\bottomrule
\end{tabular}
\end{table}



\section{Addition of new features}

\subsection{Additional features}
\section{Feature Engineering Update: From Classical Descriptors to Rich Graph Signatures}
\label{sec:feature-update}

\paragraph{Overview.}
We extend the original feature set (degree statistics, connectivity, centrality, $k$-core, robustness, limited cycle counts, Laplacian spectral summaries, planarity, symmetry, and Kirchhoff index) with several new, orthogonal families that capture higher-order structure, multiscale diffusion, community organization, and topological signals. The goal is to (i) improve expressivity for small planar graphs, (ii) increase robustness across loop orders by controlling for graph size, and (iii) enable interpretable ablations by grouping features into coherent categories.

\paragraph{Groups and motivations.}
\begin{description}
  \item[\textbf{(A) Classical structure (baseline).}]
  \emph{Degree \& density, connectivity (components/diameter/radius/ASPL/Wiener), centralities (betweenness/ closeness/ eigenvector), $k$-core indices, articulation points \& bridges, limited cycle counts (5/6-cycles), Laplacian spectrum summaries (algebraic connectivity, spectral gap, moments, first 10 eigenvalues), planarity (faces, face sizes), symmetry (automorphism order, orbits), Kirchhoff index.}
  These provide coarse geometry (how connected), flow/importance (centralities), and rigidity (algebraic connectivity, bridges).

  \item[\textbf{(B) Motifs \& graphlets (new).}]
  \emph{Triangles, wedges, 4-cycles, 4-cliques; induced 4-node graphlets (K$_{1,3}$, P$_4$, C$_4$, TailedTriangle, Diamond, K$_4$); triangle--edge embeddedness statistics (mean/std/median/q90, zero/ $\ge 2$ fractions).}
  Motifs capture \emph{localized} wiring rules and are sensitive to small structural differences that the baseline may smooth over; induced graphlets disambiguate shapes with identical degree sequences.

  \item[\textbf{(C) Adjacency-spectrum \& energy (new).}]
  \emph{Graph energy, Estrada index, adjacency spectral moments (2/3/4).}
  Complementary to Laplacian features: adjacency spectra reflect walk counts and subgraph densities; Estrada and energy summarize global communicability and overall “activity” of the network.

  \item[\textbf{(D) Heat traces \& NetLSD (new).}]
  \emph{Laplacian heat trace at multiple scales; NetLSD summary (mean/std/quantiles over $\log$-spaced times).}
  These are \emph{multiscale} diffusion fingerprints invariant to node relabeling, providing stable comparisons across graphs and enhancing cross-loop generalization.

  \item[\textbf{(E) Topological Data Analysis (new).}]
  \emph{Vietoris--Rips persistent homology on shortest-path distances: H$_0$/H$_1$ counts, total/mean/max persistence, persistence entropy, mean birth/death; Betti numbers at distance quantiles.}
  Persistence measures quantify the \emph{lifetimes} of connectivity and loop-like features as the metric thickens, offering robustness to small perturbations and complementary global shape information.

  \item[\textbf{(F) Community structure (new).}]
  \emph{Louvain modularity, community count/max size, community-size Gini, internal-edge fraction.}
  Detects meso-scale organization; modular graphs can differ in coefficients even with similar local statistics.

  \item[\textbf{(G) Distance/coverage \& local distributions (new).}]
  \emph{Effective diameter (p90 of distances), eccentricity mean/q90, degree assortativity, clustering coefficient mean/quantiles, fractions at $0$ and $1$, degree Gini.}
  Captures tail behavior and heterogeneity beyond means, which often governs extremal behaviors relevant to coefficients.

  \item[\textbf{(H) Normalized/size-invariant variants (new).}]
  \emph{Per-node/edge rates, divisions by combinatorial maxima ($\binom{n}{k}$), ratios over average degree or diameter, log-scaled automorphism size over $\log(n!)$, etc.}
  These reduce confounding by graph size and facilitate \emph{cross-loop} transfer by aligning distributions across $n$.
\end{description}

\paragraph{Why these additions help.}
(i) \emph{Higher-order locality:} Motifs and induced graphlets distinguish micro-patterns invisible to degree-based summaries.
(ii) \emph{Multiscale sensitivity:} Heat traces/NetLSD encode diffusion at multiple time scales, bridging local and global structure.
(iii) \emph{Global shape:} TDA summarizes how components and cycles persist across thresholds, complementing spectral views.
(iv) \emph{Meso-scale organization:} Community features identify clustered vs.\ homogeneous wiring, often predictive in planar settings.
(v) \emph{Stability and transfer:} Normalizations mitigate size effects and improve generalization when predicting at unseen loop orders.

\paragraph{Practical notes.}
All new groups are compatible with tree-based learners (nonlinear, interaction-aware) and support interpretable ablations: train with one group at a time, or remove a group from the full set and measure $\Delta$ performance. Normalized counterparts are recommended when mixing loop orders, while raw counts can be informative within a fixed $n$.





\subsection{results}

\begin{table}[H]
\centering
\begin{tabular}{c|c|ccccc|cc}
\hline
Pair & Loop & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Avg ROC AUC & Std Dev \\
\hline
\multicolumn{9}{c}{\textbf{Single Loops}} \\
\hline
-- & 7  & 0.905 & 0.876 & 0.798 & 0.813 & 0.840 & 0.846 & 0.040 \\
-- & 8  & 0.896 & 0.915 & 0.905 & 0.947 & 0.886 & 0.910 & 0.021 \\
-- & 9  & 0.940 & 0.936 & 0.935 & 0.941 & 0.937 & 0.938 & 0.002 \\
-- & 10 & 0.944 & 0.945 & 0.943 & 0.945 & 0.946 & 0.945 & 0.001 \\
\hline
\multicolumn{9}{c}{\textbf{Mixed Loops}} \\
\hline
7 \& 8  & 7  & 0.967 & 0.971 & 0.841 & 0.964 & 0.866 & 0.922 & 0.056 \\
        & 8  & 0.902 & 0.918 & 0.918 & 0.927 & 0.885 & 0.910 & 0.015 \\
8 \& 9  & 8  & 0.908 & 0.927 & 0.927 & 0.943 & 0.906 & 0.922 & 0.014 \\
        & 9  & 0.939 & 0.940 & 0.934 & 0.932 & 0.943 & 0.938 & 0.004 \\
7 \& 9  & 7  & 0.926 & 0.893 & 0.806 & 0.790 & 0.801 & 0.843 & 0.055 \\
        & 9  & 0.942 & 0.929 & 0.944 & 0.941 & 0.944 & 0.940 & 0.006 \\
9 \& 10 & 9  & 0.940 & 0.935 & 0.936 & 0.940 & 0.937 & 0.938 & 0.002 \\
        & 10 & 0.944 & 0.943 & 0.943 & 0.944 & 0.945 & 0.944 & 0.001 \\
\hline
\end{tabular}
\caption{ROC AUC scores for single loops and mixed loop combinations across 5 folds, with mean and standard deviation.}
\label{tab:roc_auc_subset}
\end{table}

\section{Tree Misclassification by Inclusion of Loop Order (Updated Results)}

% Preamble
\sisetup{
  detect-all,
  input-decimal-markers = {.},
  group-separator = {,}
}

% ===== Table 1: Overall by combo =====
\begin{table}[H]
\centering
\small
\caption{Overall by combo (updated results)}
\label{tab:overall-by-combo-updated}
\begin{tabular}{
  l % text column for train_loops
  S[table-format=6.0]
  S[table-format=1.6]
  S[table-format=2.0]
  S[table-format=1.6]
  S[table-format=3.0]
}
\toprule
{train\_loops} & {n\_train} & {train\_auc\_overall} & {test\_loop} & {test\_auc} & {best\_ntrees} \\
\midrule
(7, 8, 9, 10)  & 168820 & 0.968639 & 11 & 0.902108 & 600 \\
(9, 10)        & 167224 & 0.968027 & 11 & 0.900879 & 600 \\
(7, 10)        & 153416 & 0.970139 & 11 & 0.900487 & 600 \\
(8, 9, 10)     & 168656 & 0.968276 & 11 & 0.899943 & 600 \\
(7, 8, 10)     & 154848 & 0.970081 & 11 & 0.898898 & 600 \\
(10,)          & 153252 & 0.970283 & 11 & 0.898484 & 600 \\
(8, 10)        & 154684 & 0.970250 & 11 & 0.896242 & 600 \\
(7, 9, 10)     & 167388 & 0.968272 & 11 & 0.891610 & 600 \\
(7, 9)         &  14136 & 0.997864 & 11 & 0.869870 & 600 \\
(9,)           &  13972 & 0.998239 & 11 & 0.868378 & 595 \\
(7, 8, 9)      &  15568 & 0.998339 & 11 & 0.867978 & 600 \\
(8, 9)         &  15404 & 0.998303 & 11 & 0.866817 & 600 \\
(7, 8)         &   1596 & 0.990986 & 11 & 0.836198 & 103 \\
(8,)           &   1432 & 0.993116 & 11 & 0.813402 & 127 \\
(7,)           &    164 & 0.989179 & 11 & 0.699997 &  68 \\
\bottomrule
\end{tabular}
\end{table}

% ===== Table 2: Training per-loop AUCs =====
\begin{table}[H]
\centering
\small
\caption{Training per-loop AUCs (updated results, long-form)}
\label{tab:training-per-loop-updated}
\begin{tabular}{
  l % text column for train_loops
  S[table-format=2.0]
  S[table-format=6.0]
  S[table-format=1.6]
}
\toprule
{train\_loops} & {loop\_id} & {n\_train\_loop} & {train\_auc\_loop} \\
\midrule
(7,)          &  7 &    164  & 0.989179 \\
(7, 8)        &  7 &    164  & 0.987211 \\
(7, 8)        &  8 &   1432  & 0.991405 \\
(7, 8, 9)     &  7 &    164  & 1.000000 \\
(7, 8, 9)     &  8 &   1432  & 0.999686 \\
(7, 8, 9)     &  9 &  13972  & 0.998152 \\
(7, 8, 9, 10) &  7 &    164  & 0.998196 \\
(7, 8, 9, 10) &  8 &   1432  & 0.986078 \\
(7, 8, 9, 10) &  9 &  13972  & 0.971860 \\
(7, 8, 9, 10) & 10 & 153252  & 0.968128 \\
(7, 8, 10)    &  7 &    164  & 0.998360 \\
(7, 8, 10)    &  8 &   1432  & 0.986166 \\
(7, 8, 10)    & 10 & 153252  & 0.969894 \\
(7, 9)        &  7 &    164  & 0.988031 \\
(7, 9)        &  9 &  13972  & 0.997944 \\
(7, 9, 10)    &  7 &    164  & 0.999508 \\
(7, 9, 10)    &  9 &  13972  & 0.973026 \\
(7, 9, 10)    & 10 & 153252  & 0.967793 \\
(7, 10)       &  7 &    164  & 0.998360 \\
(7, 10)       & 10 & 153252  & 0.970093 \\
(8,)          &  8 &   1432  & 0.993116 \\
(8, 9)        &  8 &   1432  & 0.998997 \\
(8, 9)        &  9 &  13972  & 0.998239 \\
(8, 9, 10)    &  8 &   1432  & 0.989159 \\
(8, 9, 10)    &  9 &  13972  & 0.971840 \\
(8, 9, 10)    & 10 & 153252  & 0.967733 \\
(8, 10)       &  8 &   1432  & 0.988593 \\
(8, 10)       & 10 & 153252  & 0.970073 \\
(9,)          &  9 &  13972  & 0.998239 \\
(9, 10)       &  9 &  13972  & 0.974006 \\
(9, 10)       & 10 & 153252  & 0.967488 \\
(10,)         & 10 & 153252  & 0.970283 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Per category Ablations}
\begin{table}[H]
\centering
\caption{Ablation-style evaluation of feature categories. 
Top block: models trained on a single category. 
Middle: all-features baseline. 
Bottom: minus-one ablations (more negative $\Delta$ vs.\ ALL = more important).}
\label{tab:ablation}
\scriptsize
\begin{tabular}{lrrrr}
\toprule
\textbf{Category / Setting} & \textbf{\#Features} & \textbf{Test AUC} & \textbf{Best Trees} & $\Delta$ vs.\ ALL \\
\midrule
\multicolumn{5}{c}{\emph{Per-category models (higher AUC is better)}} \\
Centrality                  & 14  & 0.877 & 600 & -- \\
Spectral (Laplacian)        & 27  & 0.860 & 600 & -- \\
Spectral (Adjacency/Energy) & 12  & 0.799 & 599 & -- \\
Connectivity                & 13  & 0.796 & 291 & -- \\
Assortativity/Clustering    &  8  & 0.765 & 600 & -- \\
Community                   &  5  & 0.752 & 600 & -- \\
Motifs (counts)             & 15  & 0.744 & 600 & -- \\
Symmetry                    &  7  & 0.740 & 274 & -- \\
Motifs (induced-4)          & 13  & 0.719 & 600 & -- \\
Planarity                   &  5  & 0.638 &  92 & -- \\
Basic                       & 12  & 0.630 &  94 & -- \\
Cycle                       &  2  & 0.624 & 212 & -- \\
TDA                         & 38  & 0.547 &  46 & -- \\
Robustness                  &  4  & 0.500 & 229 & -- \\
Core                        &  2  & 0.500 &  14 & -- \\
(Other/Unassigned)          &  1  & 0.499 & 394 & -- \\
\midrule
\multicolumn{5}{c}{\emph{All-features baseline}} \\
ALL features                & 178 & 0.902 & 600 & reference \\
\midrule
\multicolumn{5}{c}{\emph{Minus-one ablations (more negative $\Delta$ = more important)}} \\
Assortativity/Clustering    & 170 & 0.882 & 600 & -0.020 \\
Centrality                  & 164 & 0.887 & 600 & -0.015 \\
Spectral (Laplacian)        & 151 & 0.887 & 600 & -0.015 \\
Community                   & 173 & 0.893 & 600 & -0.009 \\
Connectivity                & 165 & 0.894 & 600 & -0.008 \\
Motifs (induced-4)          & 165 & 0.896 & 600 & -0.007 \\
Symmetry                    & 171 & 0.896 & 600 & -0.006 \\
Basic                       & 166 & 0.898 & 600 & -0.004 \\
(Other/Unassigned)          & 177 & 0.899 & 600 & -0.003 \\
Robustness                  & 174 & 0.899 & 600 & -0.003 \\
Motifs (counts)             & 163 & 0.899 & 600 & -0.003 \\
Cycle                       & 176 & 0.899 & 600 & -0.003 \\
TDA                         & 140 & 0.900 & 600 & -0.002 \\
Planarity                   & 173 & 0.901 & 600 & -0.001 \\
Core                        & 176 & 0.901 & 600 & -0.001 \\
Spectral (Adjacency/Energy) & 166 & 0.902 & 600 & -0.000 \\
\bottomrule
\end{tabular}
\end{table}



\section{adding in 5 motifs}
In this next section we add a selection of new features surroounding the 5 motifs.

\section*{5-node / 5-cycle Motif Features}

\paragraph{5-cycle (\texttt{Motif\_5\_cycles})}
A \emph{5-cycle} is a simple ring of five distinct nodes:
\[
A \!-\! B \!-\! C \!-\! D \!-\! E \!-\! A,
\]
with each node connected to exactly its two neighbors and \emph{no chords} (no shortcuts across the ring).
The code counts how many such rings appear in the graph.

\paragraph{Normalized 5-cycle counts}
To compare graphs of different sizes, the raw count is normalized by the number of 5-node choices:
\[
\texttt{Motif\_5\_cycles\_per\_Cn5} \;=\; \frac{\texttt{Motif\_5\_cycles}}{\binom{n}{5}}.
\]
There is also a density-style normalization against (theoretical) maximum in a complete graph:
\[
\texttt{Motif\_5\_cycles\_per\_Kn}.
\]

\paragraph{5-clique (\texttt{Motif\_5\_cliques})}
A \emph{5-clique} is a fully connected set of five nodes (every pair has an edge). The code counts how many such fully interlinked 5-node groups exist. A size-normalized version is:
\[
\texttt{Motif\_5\_cliques\_per\_Cn5} \;=\; \frac{\texttt{Motif\_5\_cliques}}{\binom{n}{5}}.
\]

\paragraph{Induced 5-node motifs (\texttt{Motif\_induced5\_*})}
Beyond cycles and cliques, there are many possible \emph{connected} shapes on five nodes (e.g., a square with a tail, a ``house'' shape, etc.). For each 5-node subset, the code forms the \emph{induced} subgraph (include exactly those edges that exist among the five nodes in the original graph). If that induced subgraph is connected, it is classified (via Weisfeiler--Lehman hashing) into a canonical type labeled
\[
\texttt{G5\_00}, \texttt{G5\_01}, \ldots
\]
and counted in columns like \texttt{Motif\_induced5\_G5\_xx}. Each also has a normalized version:
\[
\texttt{Motif\_induced5\_G5\_xx\_per\_Cn5} \;=\; \frac{\texttt{Motif\_induced5\_G5\_xx}}{\binom{n}{5}}.
\]

\paragraph{Connected 5-sets fraction}
The script also records how often a random 5-node choice yields a connected induced subgraph:
\[
\texttt{Motif\_induced\_connected\_per\_5set} \;=\; \frac{\#\{\text{connected induced 5-node subgraphs}\}}{\binom{n}{5}}.
\]

\paragraph{Why ``induced'' matters}
``Induced'' means the pattern must match \emph{exactly} the edges among the five nodes---no missing and no extra edges. For example, a clean 5-cycle differs from a 5-cycle with a chord; these are counted as different motifs.

\paragraph{Sampling and performance}
Enumerating all 5-node subsets is combinatorial. The flag \texttt{--ind5-sample-frac} (default $1.0$) allows sampling a fraction of 5-sets. Use the normalized columns (``\texttt{\_per\_Cn5}'') to compare across graphs or runs when sampling is used.

\paragraph{How to read the numbers}
\begin{itemize}
  \item High \texttt{Motif\_5\_cycles}: many chordless 5-node loops (``loopy'' structure).
  \item High \texttt{Motif\_5\_cliques}: many very dense 5-node pockets (tight communities).
  \item Specific \texttt{Motif\_induced5\_G5\_xx} high: that exact 5-node shape is common; compare via the normalized \texttt{\_per\_Cn5} versions.
\end{itemize}



\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{c|c|ccccc|cc}
\hline
Pair & Loop & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Avg ROC AUC & Std Dev \\
\hline
\multicolumn{9}{c}{\textbf{Single Loops}} \\
\hline
-- & 6  & 0.850 & 0.100 & 1.000 & 1.000 & 1.000 & 0.790 & 0.350 \\
-- & 7  & 0.897 & 0.868 & 0.794 & 0.817 & 0.823 & 0.840 & 0.037 \\
-- & 8  & 0.931 & 0.937 & 0.922 & 0.942 & 0.904 & 0.927 & 0.014 \\
-- & 9  & 0.955 & 0.954 & 0.952 & 0.953 & 0.955 & 0.954 & 0.001 \\
-- & 10 & 0.956 & 0.957 & 0.955 & 0.956 & 0.958 & 0.957 & 0.001 \\
\hline
\multicolumn{9}{c}{\textbf{Mixed Loops}} \\
\hline
6 \& 7  & 6  & 1.000 & 0.600 & 1.000 & 1.000 & 1.000 & 0.920 & 0.160 \\
        & 7  & 0.909 & 0.884 & 0.764 & 0.881 & 0.734 & 0.835 & 0.071 \\
7 \& 8  & 7  & 0.983 & 0.967 & 0.849 & 0.952 & 0.922 & 0.935 & 0.047 \\
        & 8  & 0.933 & 0.925 & 0.927 & 0.937 & 0.923 & 0.929 & 0.005 \\
8 \& 9  & 8  & 0.956 & 0.963 & 0.951 & 0.969 & 0.926 & 0.953 & 0.015 \\
        & 9  & 0.957 & 0.956 & 0.952 & 0.952 & 0.959 & 0.955 & 0.003 \\
9 \& 10 & 9  & 0.954 & 0.949 & 0.954 & 0.956 & 0.955 & 0.953 & 0.002 \\
        & 10 & 0.957 & 0.955 & 0.956 & 0.956 & 0.957 & 0.956 & 0.001 \\
\hline
\multicolumn{9}{c}{\textbf{Progressive Training / Transfer Tests}} \\
\hline
5,6 $\rightarrow$ 7       & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (7)  & \multicolumn{5}{c|}{--}    & 0.831 & -- \\
5,6,7 $\rightarrow$ 8     & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (8)  & \multicolumn{5}{c|}{--}    & 0.800 & -- \\
5,6,7,8 $\rightarrow$ 9   & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (9)  & \multicolumn{5}{c|}{--}    & 0.828 & -- \\
5,6,7,8,9 $\rightarrow$ 10 & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (10) & \multicolumn{5}{c|}{--}    & 0.852 & -- \\
\hline
\end{tabular}
}
\caption{ROC AUC scores for single loops, mixed loop combinations, and progressive training-transfer tests across folds, with mean and standard deviation.}
\label{tab:roc_auc_updated}
\end{table}



\section{f-graphs}
\subsection{TODO: feature explanation}

\subsection{Results}

TODO: parameter search

\begin{table}[H]
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{c|c|ccccc|cc}
\hline
Pair & Loop & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Avg ROC AUC & Std Dev \\
\hline
\multicolumn{9}{c}{\textbf{Single Loops (F-Graphs)}} \\
\hline
-- & 6  & 1.000 & 1.000 & 0.700 & 0.900 & 1.000 & 0.920 & 0.117 \\
-- & 7  & 0.853 & 0.882 & 0.796 & 0.813 & 0.920 & 0.853 & 0.045 \\
-- & 8  & 0.956 & 0.952 & 0.953 & 0.948 & 0.953 & 0.953 & 0.002 \\
-- & 9  & 0.968 & 0.970 & 0.967 & 0.970 & 0.967 & 0.968 & 0.001 \\
\hline
\multicolumn{9}{c}{\textbf{Mixed Loops (F-Graphs)}} \\
\hline
6 \& 7  & 6  & 1.000 & 1.000 & 0.900 & 0.900 & 1.000 & 0.960 & 0.049 \\
        & 7  & 0.901 & 0.835 & 0.991 & 0.848 & 0.829 & 0.881 & 0.061 \\
7 \& 8  & 7  & 0.936 & 0.981 & 0.966 & 0.914 & 0.996 & 0.958 & 0.030 \\
        & 8  & 0.949 & 0.953 & 0.953 & 0.948 & 0.960 & 0.953 & 0.004 \\
8 \& 9  & 8  & 0.973 & 0.979 & 0.974 & 0.968 & 0.974 & 0.973 & 0.003 \\
        & 9  & 0.969 & 0.970 & 0.969 & 0.967 & 0.968 & 0.969 & 0.001 \\
\hline
\multicolumn{9}{c}{\textbf{Progressive Training / Transfer Tests (F-Graphs)}} \\
\hline
5,6 $\rightarrow$ 7       & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (7)  & \multicolumn{5}{c|}{--}    & 0.820 & -- \\
5,6,7 $\rightarrow$ 8     & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (8)  & \multicolumn{5}{c|}{--}    & 0.860 & -- \\
5,6,7,8 $\rightarrow$ 9   & Train     & \multicolumn{5}{c|}{1.000} & 1.000 & -- \\
                          & Test (9)  & \multicolumn{5}{c|}{--}    & 0.910 & -- \\
\hline
\end{tabular}
}
\caption{ROC AUC scores for f-graph single loops, mixed loop combinations, and progressive training-transfer tests across folds, with mean and standard deviation.}
\label{tab:roc_auc_fgraphs}
\end{table}





\end{document}
	
